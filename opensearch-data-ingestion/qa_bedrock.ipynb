{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdb221b",
   "metadata": {},
   "source": [
    "# Ingest massive amounts of data to a Vector DB (Amazon OpenSearch)\n",
    "**_Use of Amazon OpenSearch as a vector database for storing embeddings_**\n",
    "\n",
    "This notebook works well with the `conda_python3` kernel on a SageMaker Notebook `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Overall Workflow](#Overall-Workflow)\n",
    "1. [Step 1: Setup](#Step-1:-Setup)\n",
    "1. [Step 2: Load data into OpenSearch](#Step-2:-Load-data-into-OpenSearch)\n",
    "1. [Step 3: Do a similarity search for user input to documents (embeddings) in OpenSearch](#Step-3:-Do-a-similarity-search-for-for-user-input-to-documents-(embeddings)-in-OpenSearch)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24508a4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook illustrates how to use [`langchain`](https://python.langchain.com/en/latest/index.html), AWS Bedrock and Amazon Sagemaker Processing Job to convert large amount of data into embeddings and ingest the text data along with its embeddings into an Amazon OpenSearch index.\n",
    "\n",
    "We use the pdf documents download from arXiv as the dataset to convert into embeddings. The Bedrock large language model (LLM) is to generate the embeddings. \n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "- *[The langchain OpenSearch documentation](https://python.langchain.com/en/latest/ecosystem/opensearch.html)*\n",
    "- *[Amazon OpenSearch service documentation](https://docs.aws.amazon.com/opensearch-service/index.html)*\n",
    "- *[SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)*\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: \n",
    "\n",
    "Using LLMs for information retrieval tasks (such as question-answering) requires converting the knowledge corpus as well as user questions into vector embeddings. We want to generate these vector embeddings using an LLM hosted as a Amazon Sagemaker Endpoint and store it in a vector database of choice such as Amazon OpenSearch. For converting large amounts of data (TBs or PBs) we need a scalable system which can accomplish both converting the documents into embeddings, storing them in a vector database and provide low latency similarity search\n",
    "\n",
    "- **Our approach**: \n",
    "\n",
    "1. Use Bedrock Titan FM as the LLM to generate the embeddings.\n",
    "\n",
    "1. Place the data to be corpus of data in S3 (each document is a file stored as an object in S3).\n",
    "\n",
    "1. Use a Python script that uses [langchain](https://python.langchain.com/en/latest/index.html) and [Opensearch-py](https://pypi.org/project/opensearch-py/) to ingest the data into OpenSearch. Run the script locally on this notebook for testing.\n",
    "\n",
    "1. Create a Sagemaker Processing job with `instance_count` set to > 1 (usually matching the `instance_count` for the Sagemaker Endpoint). \n",
    "\n",
    "    Each instance of the SageMaker Processing Job runs a script that does the following:\n",
    "    - Processes a subset of files from S3.\n",
    "    - Uses langchain to read the files from the local filesystem and convert it into chunks.\n",
    "    - Creates a langchain `OpenSearchVectorSearch` object. This is a wrapper around OpenSearch vector databases, allowing you to use it as a vectorestore for semantic search using approximate vector seach powered by licene, nmslib and faiss engines or using pailess scripting and script scoring functions for bruteforce vector search.\n",
    "    - Does this using Python multiprocessing to achieve parallelization even within a single processing job instance and ensure maximum use of the Sagemaker Endpoint instance's GPU.\n",
    "    > **The advantage to using langchain as a wrapper for interfacing with a vector database is that it provides a generic pattern that can be used with any LLM and any vector store. Langchain automatically uses the OpenSearch bulk ingestion API endpoint for ingesting data rather than ingesting data one record at a time. Furthermore, langchain also provides an opinionated JSON structure that includes text and metadata alongwith the embeddings itself for storing embeddings in an OpenSearch index specifically for information retrieval use-cases**.\n",
    "\n",
    "- **Our tools**: [Amazon SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/), [langchain](https://python.langchain.com/en/latest/index.html) and [Opensearch-py](https://pypi.org/project/opensearch-py/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be2981",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "The following are prerequisites that needs to be accomplised by running [this cloud formation template](./template.yaml) before running this notebook.\n",
    "- A Sagemaker Endpoint for generating embeddings.\n",
    "- An Amazon OpenSearch cluster for storing embeddings.\n",
    "    - Opensearch cluster's access credentials (username and password) stored in AWS Secrets Mananger by following steps described [here](https://docs.aws.amazon.com/secretsmanager/latest/userguide/managing-secrets.html).\n",
    "\n",
    "The overall workflow for this notebook is as follows:\n",
    "1. Install the required Python packages and store session information in local variables.\n",
    "1. Download data from source and upload to S3.\n",
    "1. Run the Python script locally to ingest a subset of data into an OpenSearch index for testing.\n",
    "1. Run Sagemaker Processing Job which reads all data from S3 and runs the same Python script as above to ingest data into OpenSearch.\n",
    "    - As part of this step we also create a custom container to package langchain and opensearch Python packages.\n",
    "1. Do a similarity search with embeddings stored in the OpenSearch index for an input query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49688c52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setup\n",
    "\n",
    "Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1cf4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  239M  100  239M    0     0  43.5M      0  0:00:05  0:00:05 --:--:-- 51.4M\n",
      "Archive:  bedrock-python-sdk.zip\n",
      "   creating: bedrock-python-sdk/reviews/\n",
      "  inflating: bedrock-python-sdk/awscli-1.29.21.tar.gz  \n",
      "  inflating: bedrock-python-sdk/AWSCLI32PY3.msi  \n",
      "  inflating: bedrock-python-sdk/manifest.json  \n",
      "  inflating: bedrock-python-sdk/AWSCLISetup.exe  \n",
      "  inflating: bedrock-python-sdk/.functional  \n",
      "  inflating: bedrock-python-sdk/awscli-bundle.zip  \n",
      "  inflating: bedrock-python-sdk/boto3-1.28.21-py3-none-any.whl  \n",
      "  inflating: bedrock-python-sdk/models-starfort-report.json  \n",
      "  inflating: bedrock-python-sdk/.unit  \n",
      "  inflating: bedrock-python-sdk/AWSCLI32.msi  \n",
      "  inflating: bedrock-python-sdk/.unit-crt  \n",
      "  inflating: bedrock-python-sdk/AWSCLI64PY3.msi  \n",
      "  inflating: bedrock-python-sdk/boto3-1.28.21.tar.gz  \n",
      "  inflating: bedrock-python-sdk/.functional-crt  \n",
      "  inflating: bedrock-python-sdk/AWSCLI64.msi  \n",
      "  inflating: bedrock-python-sdk/botocore-1.31.21.tar.gz  \n",
      "  inflating: bedrock-python-sdk/awscli-1.29.21-py3-none-any.whl  \n",
      "  inflating: bedrock-python-sdk/botocore-1.31.21-py3-none-any.whl  \n",
      "  inflating: bedrock-python-sdk/reviews/awscli-1.29.21.diff  \n",
      "  inflating: bedrock-python-sdk/reviews/botocore-1.31.21.diff  \n",
      "  inflating: bedrock-python-sdk/reviews/awscli-1.29.21.html  \n",
      "  inflating: bedrock-python-sdk/reviews/boto3-1.28.21.commits  \n",
      "  inflating: bedrock-python-sdk/reviews/botocore-1.31.21.html  \n",
      "  inflating: bedrock-python-sdk/reviews/botocore-1.31.21.commits  \n",
      "  inflating: bedrock-python-sdk/reviews/boto3-1.28.21.html  \n",
      "  inflating: bedrock-python-sdk/reviews/awscli-1.29.21.commits  \n",
      "  inflating: bedrock-python-sdk/reviews/boto3-1.28.21.diff  \n"
     ]
    }
   ],
   "source": [
    "#Download the Bedrock dependencies.\n",
    "!bash ./download-dependencies.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bff98-d47c-402d-a068-c9370dcf5ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    ./bedrock-python-sdk/awscli-*-py3-none-any.whl \\\n",
    "    ./bedrock-python-sdk/boto3-*-py3-none-any.whl \\\n",
    "   ./bedrock-python-sdk/botocore-*-py3-none-any.whl\n",
    "%pip install --quiet langchain==0.0.249 \"pypdf>=3.8,<4\"\n",
    "%pip install opensearch-py==2.2.0\n",
    "%pip install requests\n",
    "%pip install requests-aws4auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0390161-136d-4f24-9524-a6acc5c985cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import logging, sys\n",
    "from typing import List\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.processing import ScriptProcessor, FrameworkProcessor\n",
    "from sagemaker.processing import ProcessingInput\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343808f",
   "metadata": {},
   "source": [
    "Change the parameters if you would like to scrape a different website for data, customize chunk size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8576c1-56c5-44f6-b5f6-ff60e4faba67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# global constants\n",
    "APP_NAME = \"llm-rag-hackathon\"\n",
    "DATA_DIR = \"data\"\n",
    "MAX_OS_DOCS_PER_PUT = 500\n",
    "IMAGE = \"load-data-opensearch-custom\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "CHUNK_SIZE_FOR_DOC_SPLIT = 600\n",
    "CHUNK_OVERLAP_FOR_DOC_SPLIT = 20\n",
    "CREATE_OS_INDEX_HINT_FILE = \"_create_index_hint\"\n",
    "SOURCE_BUCKET = \"arxiv-docs-21082003\"\n",
    "opensearch_index=\"llm-rag-hackathon\"\n",
    "embeddings_model_endpoint_name='amazon.titan-e1t-medium'\n",
    "llm_model_id=\"amazon.titan-tg1-large\"\n",
    "BEDROCK_ENDPOINT_URL = \"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "#BEDROCK_REGION = \"us-west-2\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n",
    "logger.info(f\"aws_role={aws_role}, aws_region={aws_region}, bucket={bucket}, source bucket={SOURCE_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19d81d",
   "metadata": {},
   "source": [
    "### Read parameters from Cloud Formation stack\n",
    "\n",
    "Some of the resources needed for this notebook such as the Embeddings LLM model endpoint, the Amazon OpenSearch cluster are created outside of this notebook, typically through a cloud formation template. We now read the outputs and parameters of the cloud formation stack created from that template to get the value of these parameters. \n",
    "\n",
    "The stack name here should match the stack name you used when creating the cloud formation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if used a different name while creating the cloud formation stack then change this to match the name you used\n",
    "CFN_STACK_NAME = \"llm-rag-hackathon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda3dc",
   "metadata": {},
   "source": [
    "**If you did not use a cloud formation template for creating these resources then set the names of these resources manually in the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacks = boto3.client('cloudformation').list_stacks()\n",
    "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cfn_outputs(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def get_cfn_parameters(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    params = {}\n",
    "    for param in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Parameters']:\n",
    "        params[param['ParameterKey']] = param['ParameterValue']\n",
    "    return params\n",
    "\n",
    "if stack_found is True:\n",
    "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "    params = get_cfn_parameters(CFN_STACK_NAME)\n",
    "    logger.info(f\"cfn outputs={outputs}\\nparams={params}\")\n",
    "    opensearch_domain_endpoint = f\"https://{outputs['OpenSearchDomainEndpoint']}\"\n",
    "else:\n",
    "    logger.info(f\"cloud formation stack {CFN_STACK_NAME} not found, set parameters manually here\")\n",
    "    # REPLACE THE \"placeholder\" WITH ACTUAL VALUES IF YOU CREATED THESE RESOURCES WITHOUT USING A CLOUD FORMATION TEMPLATE\n",
    "    opensearch_domain_endpoint = \"placeholder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cf7f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load data into `OpenSearch`\n",
    "\n",
    "We are now ready to create scripts which will read data from the local directory, use langchain to create embeddings and then upload the embeddings into OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76998f14-cbd2-4684-b2b8-27ea920e727f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create directories for storing scripts and Dockerfile\n",
    "\"\"\"\n",
    "!mkdir src\n",
    "!mkdir data\n",
    "!mkdir scripts\n",
    "!mkdir container\n",
    "!mkdir container/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d722d7-0386-42de-8ca0-213fd0ab55ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dummy file called _create_index to provide a hint for opensearch index creation\n",
    "# this is needed for Sagemaker Processing Job when there are multiple instance nodes\n",
    "# all running the same code for data ingestion but only one node needs to create the index\n",
    "!touch $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE\n",
    "\n",
    "!ls $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE\n",
    "\n",
    "# upload this data to S3, to be used when we run the Sagemaker Processing Job\n",
    "#!aws s3 cp --recursive $DATA_DIR/ s3://$bucket/$app_name/$DOMAIN\n",
    "\n",
    "#TODO - Ensure the Sagemaker role has access to the custom bucket. - Update IAM role. LLMAppsBlogIAMRole\n",
    "!aws s3 cp $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE s3://$SOURCE_BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ee897",
   "metadata": {},
   "source": [
    "### Script to load data into OpenSearch\n",
    "\n",
    "This script puts everything together, it divides the documents into chunks, then uses the langchain package to create embeddings and then ingests the data into OpenSearch using `OpenSearchVectorSearch`. \n",
    "\n",
    "To keep things simple the chunks size is set to a fixed length of 500 tokens, with an overlap of 30 tokens. The langchain `OpenSearchVectorSearch` provides a wrapper over the `opensearch-py` package. It uses the `/_bulk` API endpoint for ingesting multiple records in a single PUT request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f60f43-f9dd-43a5-9e79-104a6f8ee867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile container/load_data_into_opensearch.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# this is needed because the credentials.py and sm_helper.py\n",
    "# are in /code directory of the custom container we are going \n",
    "# to create for Sagemaker Processing Job\n",
    "sys.path.insert(1, '/code')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "import sagemaker, boto3, json\n",
    "from typing import List, Tuple\n",
    "from sagemaker.session import Session\n",
    "from opensearchpy.client import OpenSearch\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# global constants\n",
    "MAX_OS_DOCS_PER_PUT = 500\n",
    "TOTAL_INDEX_CREATION_WAIT_TIME = 60\n",
    "PER_ITER_SLEEP_TIME = 5\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n",
    "\n",
    "\n",
    "def check_if_index_exists(opensearch_domain_endpoint, index_name, region, host, http_auth) -> OpenSearch:\n",
    "    \n",
    "    aos_client = OpenSearch(\n",
    "        hosts = [{'host': opensearch_domain_endpoint.replace(\"https://\", \"\"), 'port': 443}],\n",
    "        http_auth = http_auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    exists = aos_client.indices.exists(index_name)\n",
    "    logger.info(f\"index_name={index_name}, exists={exists}\")\n",
    "    return exists\n",
    "\n",
    "    \n",
    "def process_shard(shard, embeddings_model_endpoint_name, aws_region, os_index_name, os_domain_ep, os_http_auth) -> int: \n",
    "    logger.info(f'Starting process_shard of {len(shard)} chunks.')\n",
    "    st = time.time()\n",
    "    embeddings = bedrock_embeddings\n",
    "    \n",
    "    docsearch = OpenSearchVectorSearch(index_name=os_index_name,\n",
    "                                       embedding_function=embeddings,\n",
    "                                       opensearch_url=os_domain_ep,\n",
    "                                       http_auth=os_http_auth,\n",
    "                                       is_aoss=False,\n",
    "                                       use_ssl = True,\n",
    "                                       verify_certs = True,\n",
    "                                       connection_class = RequestsHttpConnection)    \n",
    "    docsearch.add_documents(documents=shard)\n",
    "    et = time.time() - st\n",
    "    logger.info(f'Shard completed in {et} seconds.')\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--opensearch-cluster-domain\", type=str, default=None)\n",
    "    parser.add_argument(\"--opensearch-secretid\", type=str, default=None)\n",
    "    parser.add_argument(\"--opensearch-index-name\", type=str, default=None)\n",
    "    parser.add_argument(\"--aws-region\", type=str, default=\"us-west-2\")\n",
    "    parser.add_argument(\"--embeddings-model-endpoint-name\", type=str, default=None)\n",
    "    parser.add_argument(\"--chunk-size-for-doc-split\", type=int, default=500)\n",
    "    parser.add_argument(\"--chunk-overlap-for-doc-split\", type=int, default=30)\n",
    "    parser.add_argument(\"--input-data-dir\", type=str, default=\"/opt/ml/processing/input_data\")\n",
    "    parser.add_argument(\"--process-count\", type=int, default=1)\n",
    "    parser.add_argument(\"--create-index-hint-file\", type=str, default=\"_create_index_hint\")\n",
    "    parser.add_argument(\"--bedrock-endpoint-url\", type=str, default=None)\n",
    "    parser.add_argument(\"--llm-model-id\", type=str, default=None)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(\"Received arguments {}\".format(args))\n",
    "    # list all the files\n",
    "    files = glob.glob(os.path.join(args.input_data_dir, \"*.*\"))\n",
    "    logger.info(f\"there are {len(files)} files to process in the {args.input_data_dir} folder\")\n",
    "    \n",
    "    loader = PyPDFDirectoryLoader(path=args.input_data_dir, silent_errors=True)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=args.chunk_size_for_doc_split,\n",
    "        chunk_overlap=args.chunk_overlap_for_doc_split,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Stage one: read all the docs, split them into chunks. \n",
    "    st = time.time() \n",
    "    logger.info('Loading documents ...')\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "    # avg_char_count_pre = avg_doc_length(docs)\n",
    "    # avg_char_count_post = avg_doc_length(docs)\n",
    "    # print(f'Average length among {len(docs)} documents loaded is {avg_char_count_pre} characters.')\n",
    "    # print(f'After the split we have {len(docs)} documents more than the original {len(files)}.')\n",
    "    # print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')\n",
    "    \n",
    "    # add a custom metadata field, such as timestamp\n",
    "    for doc in docs:\n",
    "        doc.metadata['timestamp'] = time.time()\n",
    "        doc.metadata['embeddings_model'] = args.embeddings_model_endpoint_name\n",
    "    chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
    "    et = time.time() - st\n",
    "    logger.info(f'Time taken: {et} seconds. {len(chunks)} chunks generated') \n",
    "    \n",
    "    \n",
    "    db_shards = (len(chunks) // MAX_OS_DOCS_PER_PUT) + 1\n",
    "    logger.info(f'Loading chunks into vector store ... using {db_shards} shards')\n",
    "    st = time.time()\n",
    "    shards = np.array_split(chunks, db_shards)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    #OpenSearch Auth\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, args.aws_region, service, session_token=credentials.token)\n",
    "    \n",
    "    #Bedrock embeddings\n",
    "    BEDROCK_ENDPOINT_URL = args.bedrock_endpoint_url\n",
    "    BEDROCK_REGION = args.aws_region\n",
    "  \n",
    "    boto3_bedrock = boto3.client(\n",
    "         service_name='bedrock',\n",
    "         region_name=BEDROCK_REGION,\n",
    "         endpoint_url=BEDROCK_ENDPOINT_URL\n",
    "    )\n",
    "    \n",
    "    model_kwargs =  { \n",
    "        \"maxTokenCount\": 8192, \n",
    "        \"stopSequences\": [], \n",
    "        \"temperature\": 0, \n",
    "        \"topP\": 1 \n",
    "    }\n",
    "\n",
    "\n",
    "    llm = Bedrock(model_id=args.llm_model_id, client=boto3_bedrock, model_kwargs=model_kwargs)\n",
    "    bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)\n",
    "    \n",
    "    # first check if index exists, if it does then call the add_documents function\n",
    "    # otherwise call the from_documents function which would first create the index\n",
    "    # and then do a bulk add. Both add_documents and from_documents do a bulk add\n",
    "    # but it is important to call from_documents first so that the index is created\n",
    "    # correctly for K-NN\n",
    "    logger.info(f'Checking if index exists')\n",
    "    index_exists = check_if_index_exists(args.opensearch_cluster_domain,\n",
    "                                         args.opensearch_index_name,\n",
    "                                         args.aws_region,\n",
    "                                         args.opensearch_cluster_domain,\n",
    "                                         awsauth)\n",
    "    \n",
    "    embeddings =  bedrock_embeddings\n",
    "    \n",
    "    \n",
    "    if index_exists is False:\n",
    "        # create an index if the create index hint file exists\n",
    "        path = os.path.join(args.input_data_dir, args.create_index_hint_file)\n",
    "        if os.path.isfile(path) is True:\n",
    "            logger.info(f\"index {args.opensearch_index_name} does not exist but {path} file is present so will create index\")\n",
    "             \n",
    "            \n",
    "            # by default langchain would create a k-NN index and the embeddings would be ingested as a k-NN vector type\n",
    "            docsearch = OpenSearchVectorSearch.from_documents(index_name=args.opensearch_index_name,\n",
    "                                                              documents=shards[0],\n",
    "                                                              embedding=embeddings,\n",
    "                                                              opensearch_url=args.opensearch_cluster_domain,\n",
    "                                                              use_ssl = True,\n",
    "                                                              verify_certs = True,  \n",
    "                                                              timeout = 300,\n",
    "                                                              connection_class = RequestsHttpConnection,\n",
    "                                                              http_auth=awsauth)\n",
    "            # we now need to start the loop below for the second shard\n",
    "            shard_start_index = 1  \n",
    "        else:\n",
    "            logger.info(f\"index {args.opensearch_index_name} does not exist and {path} file is not present, \"\n",
    "                        f\"will wait for some other node to create the index\")\n",
    "            shard_start_index = 0\n",
    "            # start a loop to wait for index creation by another node\n",
    "            time_slept = 0\n",
    "            while True:\n",
    "                logger.info(f\"index {args.opensearch_index_name} still does not exist, sleeping...\")\n",
    "                time.sleep(PER_ITER_SLEEP_TIME)\n",
    "                index_exists = check_if_index_exists(args.opensearch_cluster_domain,\n",
    "                                                     args.opensearch_index_name,\n",
    "                                                     args.aws_region,\n",
    "                                                     args.opensearch_cluster_domain,\n",
    "                                                     awsauth)\n",
    "                if index_exists is True:\n",
    "                    logger.info(f\"index {args.opensearch_index_name} now exists\")\n",
    "                    break\n",
    "                time_slept += PER_ITER_SLEEP_TIME\n",
    "                if time_slept >= TOTAL_INDEX_CREATION_WAIT_TIME:\n",
    "                    logger.error(f\"time_slept={time_slept} >= {TOTAL_INDEX_CREATION_WAIT_TIME}, not waiting anymore for index creation\")\n",
    "                    break\n",
    "                \n",
    "    else:\n",
    "        logger.info(f\"index={args.opensearch_index_name} does exist, going to call add_documents\")\n",
    "        shard_start_index = 0\n",
    "        \n",
    "    with mp.Pool(processes = args.process_count) as pool:\n",
    "        results = pool.map(partial(process_shard,\n",
    "                                   embeddings_model_endpoint_name=args.embeddings_model_endpoint_name,\n",
    "                                   aws_region=args.aws_region,\n",
    "                                   os_index_name=args.opensearch_index_name,\n",
    "                                   os_domain_ep=args.opensearch_cluster_domain,\n",
    "                                   os_http_auth=awsauth),\n",
    "                           shards[shard_start_index:])\n",
    "    \n",
    "    t2 = time.time()\n",
    "    logger.info(f'run time in seconds: {t2-t1:.2f}')\n",
    "    logger.info(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d27e3-b4fd-4d72-bb28-4334998c063e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the data in a OpenSearch index ( Local Mode )\n",
    "import subprocess\n",
    "def run_cmd(cmd: str) -> None:\n",
    "    \"\"\"\n",
    "    Run a shell command. This function exists because often it is \n",
    "    cumbersome to run a shell command that takes parameters which \n",
    "    are Python variables.\n",
    "    \"\"\"\n",
    "    MAX_OUTPUT_LEN = 20000\n",
    "    logger.info(f\"run_cmd, going to run cmd=\\\"{cmd}\\\"\")\n",
    "\n",
    "    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    errcode = p.returncode\n",
    "\n",
    "    # result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    out = out.decode(\"utf-8\") \n",
    "    err = err.decode(\"utf-8\")\n",
    "    if len(out) > MAX_OUTPUT_LEN:\n",
    "        out = out[:-MAX_OUTPUT_LEN]\n",
    "    if len(err) > MAX_OUTPUT_LEN:\n",
    "        err = err[:-MAX_OUTPUT_LEN]\n",
    "    logger.info(f\"errcode={errcode}, out={out}, err={err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ddd2f7-5fc0-46fb-b2cf-c5b40374641b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"\"\"python container/load_data_into_opensearch.py --opensearch-cluster-domain {opensearch_domain_endpoint} \\\n",
    "--opensearch-index-name {opensearch_index} \\\n",
    "--aws-region {aws_region} \\\n",
    "--embeddings-model-endpoint-name {embeddings_model_endpoint_name} \\\n",
    "--chunk-size-for-doc-split 500 \\\n",
    "--chunk-overlap-for-doc-split 50 \\\n",
    "--input-data-dir {DATA_DIR} \\\n",
    "--create-index-hint-file {CREATE_OS_INDEX_HINT_FILE} \\\n",
    "--bedrock-endpoint-url {BEDROCK_ENDPOINT_URL} \\\n",
    "--llm-model-id {llm_model_id} \\\n",
    "--process-count 2\n",
    "\"\"\"\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdac398",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the data in a `OpenSearch` index via SageMaker Processing Job (Distributed mode)\n",
    "\n",
    "We now have a working script that is able to ingest data into an OpenSearch index. But for this to work for massive amounts of data we need to scale up the processing by running this code in a distributed fashion. We will do this using Sagemkaer Processing Job. This involves the following steps:\n",
    "\n",
    "1. Create a custom container in which we will install the `langchain` and `opensearch-py` packges and then upload this container image to Amazon Elastic Container Registry (ECR).\n",
    "2. Use the Sagemaker `ScriptProcessor` class to create a Sagemaker Processing job that will run on multiple nodes.\n",
    "    - The data files available in S3 are automatically distributed across in the Sagemaker Processing Job instances by setting `s3_data_distribution_type='ShardedByS3Key'` as part of the `ProcessingInput` provided to the processing job.\n",
    "    - Each node processes a subset of the files and this brings down the overall time required to ingest the data into Opensearch.\n",
    "    - Each node also uses Python `multiprocessing` to internally also parallelize the file processing. Thus, **there are two levels of parallelization happening, one at the cluster level where individual nodes are distributing the work (files) amongst themselves and another at the node level where the files in a node are also split between multiple processes running on the node**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcd8ee",
   "metadata": {},
   "source": [
    "### Create custom container\n",
    "\n",
    "We will now create a container locally and push the container image to ECR. **The container creation process takes about 1 minute**.\n",
    "\n",
    "1. The container include all the Python packages we need i.e. `langchain`, `opensearch-py`, `sagemaker` and `beautifulsoup4`.\n",
    "1. The container also includes the `credentials.py` script for retrieving credentials from Secrets Manager and `sm_helper.py` for helping to create SageMaker endpoint classes that langchain uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f719427-a448-43f1-90f3-4ad9259e7b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile container/Dockerfile\n",
    "\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         curl \\\n",
    "         unzip \\\n",
    "         python3-pip \\\n",
    "         python3-setuptools \\\n",
    "         nginx \\\n",
    "         ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
    "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
    "\n",
    "# pip leaves the install caches populated which uses a \n",
    "# significant amount of space. These optimizations save a fair \n",
    "# amount of space in the image, which reduces start up time.\n",
    "\n",
    "# download bedrock dependencies\n",
    "RUN curl https://d2eo22ngex1n9g.cloudfront.net/Documentation/SDK/bedrock-python-sdk.zip --output bedrock-python-sdk.zip && unzip bedrock-python-sdk.zip -d bedrock-python-sdk\n",
    "RUN pip --no-cache-dir install langchain==0.0.249 opensearch-py==2.2.0 sagemaker==2.148.0 beautifulsoup4==4.12.2 \"faiss-cpu>=1.7,<2\" \"pypdf>=3.8,<4\" requests requests-aws4auth\n",
    "RUN pip install --no-build-isolation --force-reinstall \\\n",
    "    ./bedrock-python-sdk/awscli-*-py3-none-any.whl \\\n",
    "    ./bedrock-python-sdk/boto3-*-py3-none-any.whl \\\n",
    "   ./bedrock-python-sdk/botocore-*-py3-none-any.whl\n",
    "\n",
    "ADD utils /code/utils\n",
    "COPY load_data_into_opensearch.py /code/\n",
    "RUN rm -rf bedrock-python-sdk\n",
    "\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d18ac0-5deb-4fb6-9054-35816b1840b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/build_and_push.sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "# The argument to this script are the path to the Dockerfile, the image name and tag and the aws-region\n",
    "# in which the container is to be created. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "\n",
    "# override the built-in echo so that we can have a nice timestamped trace\n",
    "echo () {\n",
    "    builtin echo \"$(date +'[%m-%d %H:%M:%S]'):\" \"$@\"\n",
    "}\n",
    "\n",
    "if [ \"$#\" -eq 4 ]; then\n",
    "    dlc_account_id=$(aws sts get-caller-identity | jq .Account)\n",
    "    path_to_dockerfile=$1\n",
    "    image=$2\n",
    "    tag=$3\n",
    "    region=$4\n",
    "    \n",
    "else\n",
    "    echo \"missing mandatory command line arguments, see usage...\"\n",
    "    echo \"usage: $0 </path/to/Dockerfile> $1 <image-repo> $2 <image-tag> $3 <aws-region>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    exit 255\n",
    "fi\n",
    "\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:${tag}\"\n",
    "echo the full image name would be ${fullname}\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${region} --repository-names \"${image}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"creating ECR repository : ${fullname} \"\n",
    "    aws ecr create-repository --region ${region} --repository-name \"${image}\" > /dev/null\n",
    "else\n",
    "    echo \"${image} repo already exists in ECR\"\n",
    "fi\n",
    "\n",
    "# move to path of dockerfile\n",
    "cd ${path_to_dockerfile}\n",
    "\n",
    "# get credentials to login to ECR and, build and tag the image\n",
    "# note the use of DOCKER_BUILDKIT=1, this is needed for some mount instructions in the Dockerfile\n",
    "echo \"going to start a docker build, image=${image}, using Dockerfile=${path_to_dockerfile}\"\n",
    "aws ecr get-login-password --region ${region} \\\n",
    "| docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "DOCKER_BUILDKIT=1 docker build . -t ${image}  --build-arg dlc_account_id=${dlc_account_id} --build-arg region=${region}\n",
    "docker tag ${image} ${fullname}\n",
    "echo ${image} created\n",
    "\n",
    "# push the image to ECR\n",
    "cmd=\"aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "echo going to run \\\"${cmd}\\\" to login to ECR\n",
    "${cmd}\n",
    "\n",
    "cmd=\"docker push ${fullname}\"\n",
    "echo going to run \\\"${cmd}\\\" to push image to ecr\n",
    "${cmd}\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Amazon ECR URI: ${fullname}\"\n",
    "else\n",
    "    echo \"Error: Image ${fullname} build and push failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"all done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687202b3-3b51-4beb-8c3e-0269e85ba7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run script to build docker custom containe image and push it to ECR \n",
    "# Set region and sagemaker URI variables \n",
    "session = boto3.session.Session()\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "logger.info(f\"region={aws_region}, account_id={account_id}\")\n",
    "!bash scripts/build_and_push.sh $(pwd)/container $IMAGE $IMAGE_TAG $aws_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad4b50",
   "metadata": {},
   "source": [
    "### Create and run the Sagemaker Processing Job\n",
    "\n",
    "Now we will run the Sagemaker Processing Job to ingest the data into OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f45356-13fe-49f9-b2a0-09ac9dd54b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup the parameters for the job\n",
    "base_job_name = f\"{APP_NAME}-job\"\n",
    "tags = [{\"Key\": \"data\", \"Value\": \"embeddings-for-llm-apps\"}]\n",
    "\n",
    "# use the custom container we just created\n",
    "image_uri = f\"{account_id}.dkr.ecr.{aws_region}.amazonaws.com/{IMAGE}:{IMAGE_TAG}\"\n",
    "\n",
    "# instance type and count determined via trial and error: how much overall processing time\n",
    "# and what compute cost works best for your use-case\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 3\n",
    "logger.info(f\"base_job_name={base_job_name}, tags={tags}, image_uri={image_uri}, instance_type={instance_type}, instance_count={instance_count}\")\n",
    "\n",
    "# setup the ScriptProcessor with the above parameters\n",
    "processor = ScriptProcessor(base_job_name=base_job_name,\n",
    "                            image_uri=image_uri,\n",
    "                            role=aws_role,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            command=[\"python3\"],\n",
    "                            tags=tags)\n",
    "\n",
    "# setup input from S3, note the ShardedByS3Key, this ensures that \n",
    "# each instance gets a random and equal subset of the files in S3.\n",
    "inputs = [ProcessingInput(source=f\"s3://{SOURCE_BUCKET}\",\n",
    "                          destination='/opt/ml/processing/input_data',\n",
    "                          s3_data_distribution_type='ShardedByS3Key',\n",
    "                          s3_data_type='S3Prefix')]\n",
    "\n",
    "\n",
    "logger.info(f\"creating an opensearch index with name={opensearch_index}\")\n",
    "# ready to run the processing job\n",
    "st = time.time()\n",
    "processor.run(code=\"container/load_data_into_opensearch.py\",\n",
    "              inputs=inputs,\n",
    "              outputs=[],\n",
    "              arguments=[\"--opensearch-cluster-domain\", opensearch_domain_endpoint,\n",
    "                         \"--opensearch-index-name\", opensearch_index,\n",
    "                         \"--aws-region\", aws_region,\n",
    "                         \"--embeddings-model-endpoint-name\", embeddings_model_endpoint_name,\n",
    "                         \"--chunk-size-for-doc-split\", str(CHUNK_SIZE_FOR_DOC_SPLIT),\n",
    "                         \"--chunk-overlap-for-doc-split\", str(CHUNK_OVERLAP_FOR_DOC_SPLIT),\n",
    "                         \"--input-data-dir\", \"/opt/ml/processing/input_data\",\n",
    "                         \"--create-index-hint-file\", CREATE_OS_INDEX_HINT_FILE,\n",
    "                         \"--bedrock-endpoint-url\", BEDROCK_ENDPOINT_URL,\n",
    "                         \"--llm-model-id\", llm_model_id,\n",
    "                         \"--process-count\", \"2\"])\n",
    "time_taken = time.time() - st\n",
    "logger.info(f\"processing job completed, total time taken={time_taken}s\")\n",
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "logger.info(preprocessing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626157dd",
   "metadata": {},
   "source": [
    "## Step 3: Do a similarity search for for user input to documents (embeddings) in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2bc60-921e-4bb1-b7e7-db4c9b4165fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a similarity search for user input to documents ( embeddings) in OpenSearch\n",
    "import boto3\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "#OpenSearch Auth\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, aws_region, service, session_token=credentials.token)\n",
    "\n",
    "#Bedrock embeddings\n",
    "BEDROCK_ENDPOINT_URL = \"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "BEDROCK_REGION = \"us-west-2\"\n",
    "\n",
    "\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    endpoint_url=BEDROCK_ENDPOINT_URL,\n",
    "    region_name=BEDROCK_REGION,\n",
    ")\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"maxTokenCount\": 8192, \n",
    "    \"stopSequences\": [], \n",
    "    \"temperature\": 0, \n",
    "    \"topP\": 1 \n",
    "}\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"amazon.titan-tg1-large\", client=boto3_bedrock, model_kwargs=model_kwargs)\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch(index_name=opensearch_index,\n",
    "                                   embedding_function=bedrock_embeddings,\n",
    "                                   opensearch_url=opensearch_domain_endpoint,\n",
    "                                   http_auth=awsauth,\n",
    "                                   is_aoss=False,\n",
    "                                   use_ssl = True,\n",
    "                                   verify_certs = True,\n",
    "                                   connection_class = RequestsHttpConnection) \n",
    "\n",
    "q = \"knowledge from language models\"\n",
    "#docs = docsearch.similarity_search(q, k=3, search_type=\"script_scoring\", space_type=\"cosinesimil\")\n",
    "docs = docsearch.similarity_search(q, k=3, search_type=\"approximate_search\", space_type=\"cosinesimil\")\n",
    "for doc in docs:\n",
    "    logger.info(\"----------\")\n",
    "    logger.info(f\"content=\\\"{doc.page_content}\\\",\\nmetadata=\\\"{doc.metadata}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797ddcc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the CloudFormation template used to create the IAM role and SageMaker notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb61b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "In this notebook we were able to see how to use the BEdrock Titam model to generate embeddings and then ingest those embeddings into OpenSearch and finally do a similarity search for user input to the documents (embeddings) stored in OpenSearch. We used langchain as an abstraction layer to talk to both the SageMaker Endpoint as well as OpenSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
