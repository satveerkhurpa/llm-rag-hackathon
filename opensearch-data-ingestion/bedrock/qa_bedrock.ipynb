{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acdb221b",
   "metadata": {},
   "source": [
    "# Ingest massive amounts of data to a Vector DB (Amazon OpenSearch)\n",
    "**_Use of Amazon OpenSearch as a vector database for storing embeddings_**\n",
    "\n",
    "This notebook works well with the `conda_python3` kernel on a SageMaker Notebook `ml.t3.xlarge` instance.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective)\n",
    "1. [Background](#Background-(Problem-Description-and-Approach))\n",
    "1. [Overall Workflow](#Overall-Workflow)\n",
    "1. [Step 1: Setup](#Step-1:-Setup)\n",
    "1. [Step 2: Load data into OpenSearch](#Step-2:-Load-data-into-OpenSearch)\n",
    "1. [Step 3: Do a similarity search for user input to documents (embeddings) in OpenSearch](#Step-3:-Do-a-similarity-search-for-for-user-input-to-documents-(embeddings)-in-OpenSearch)\n",
    "1. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24508a4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook illustrates how to use [`langchain`](https://python.langchain.com/en/latest/index.html), AWS Bedrock and Amazon Sagemaker Processing Job to convert large amount of data into embeddings and ingest the text data along with its embeddings into an Amazon OpenSearch index.\n",
    "\n",
    "We use the pdf documents download from arXiv as the dataset to convert into embeddings. The Bedrock large language model (LLM) is to generate the embeddings. \n",
    "\n",
    "To understand the code, you might also find it useful to refer to:\n",
    "\n",
    "- *[The langchain OpenSearch documentation](https://python.langchain.com/en/latest/ecosystem/opensearch.html)*\n",
    "- *[Amazon OpenSearch service documentation](https://docs.aws.amazon.com/opensearch-service/index.html)*\n",
    "- *[SageMaker Processing Job](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html)*\n",
    "---\n",
    "\n",
    "## Background (Problem Description and Approach)\n",
    "\n",
    "- **Problem statement**: \n",
    "\n",
    "Using LLMs for information retrieval tasks (such as question-answering) requires converting the knowledge corpus as well as user questions into vector embeddings. We want to generate these vector embeddings using an LLM hosted as a Amazon Sagemaker Endpoint and store it in a vector database of choice such as Amazon OpenSearch. For converting large amounts of data (TBs or PBs) we need a scalable system which can accomplish both converting the documents into embeddings, storing them in a vector database and provide low latency similarity search\n",
    "\n",
    "- **Our approach**: \n",
    "\n",
    "1. Use Bedrock Titan FM as the LLM to generate the embeddings.\n",
    "\n",
    "1. Place the data to be corpus of data in S3 (each document is a file stored as an object in S3).\n",
    "\n",
    "1. Use a Python script that uses [langchain](https://python.langchain.com/en/latest/index.html) and [Opensearch-py](https://pypi.org/project/opensearch-py/) to ingest the data into OpenSearch. Run the script locally on this notebook for testing.\n",
    "\n",
    "1. Create a Sagemaker Processing job with `instance_count` set to > 1 (usually matching the `instance_count` for the Sagemaker Endpoint). \n",
    "\n",
    "    Each instance of the SageMaker Processing Job runs a script that does the following:\n",
    "    - Processes a subset of files from S3.\n",
    "    - Uses langchain to read the files from the local filesystem and convert it into chunks.\n",
    "    - Creates a langchain `OpenSearchVectorSearch` object. This is a wrapper around OpenSearch vector databases, allowing you to use it as a vectorestore for semantic search using approximate vector seach powered by licene, nmslib and faiss engines or using pailess scripting and script scoring functions for bruteforce vector search.\n",
    "    - Does this using Python multiprocessing to achieve parallelization even within a single processing job instance and ensure maximum use of the Sagemaker Endpoint instance's GPU.\n",
    "    > **The advantage to using langchain as a wrapper for interfacing with a vector database is that it provides a generic pattern that can be used with any LLM and any vector store. Langchain automatically uses the OpenSearch bulk ingestion API endpoint for ingesting data rather than ingesting data one record at a time. Furthermore, langchain also provides an opinionated JSON structure that includes text and metadata alongwith the embeddings itself for storing embeddings in an OpenSearch index specifically for information retrieval use-cases**.\n",
    "\n",
    "- **Our tools**: [Amazon SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/), [langchain](https://python.langchain.com/en/latest/index.html) and [Opensearch-py](https://pypi.org/project/opensearch-py/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be2981",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overall Workflow\n",
    "\n",
    "**Prerequisite**\n",
    "\n",
    "The following are prerequisites that needs to be accomplised by running [this cloud formation template](./template.yaml) before running this notebook.\n",
    "- A Sagemaker Endpoint for generating embeddings.\n",
    "- An Amazon OpenSearch cluster for storing embeddings.\n",
    "    - Opensearch cluster's access credentials (username and password) stored in AWS Secrets Mananger by following steps described [here](https://docs.aws.amazon.com/secretsmanager/latest/userguide/managing-secrets.html).\n",
    "\n",
    "The overall workflow for this notebook is as follows:\n",
    "1. Install the required Python packages and store session information in local variables.\n",
    "1. Download data from source and upload to S3.\n",
    "1. Run the Python script locally to ingest a subset of data into an OpenSearch index for testing.\n",
    "1. Run Sagemaker Processing Job which reads all data from S3 and runs the same Python script as above to ingest data into OpenSearch.\n",
    "    - As part of this step we also create a custom container to package langchain and opensearch Python packages.\n",
    "1. Do a similarity search with embeddings stored in the OpenSearch index for an input query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49688c52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Setup\n",
    "\n",
    "Install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cf4f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download the Bedrock dependencies.\n",
    "!bash ./download-dependencies.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4bff98-d47c-402d-a068-c9370dcf5ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./bedrock-python-sdk/awscli-1.29.21-py3-none-any.whl\n",
      "Processing ./bedrock-python-sdk/boto3-1.28.21-py3-none-any.whl\n",
      "Processing ./bedrock-python-sdk/botocore-1.31.21-py3-none-any.whl\n",
      "Collecting docutils<0.17,>=0.10 (from awscli==1.29.21)\n",
      "  Using cached docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from awscli==1.29.21)\n",
      "  Obtaining dependency information for s3transfer<0.7.0,>=0.6.0 from https://files.pythonhosted.org/packages/d9/17/a3b666f5ef9543cfd3c661d39d1e193abb9649d0cfbbfee3cf3b51d5af02/s3transfer-0.6.2-py3-none-any.whl.metadata\n",
      "  Using cached s3transfer-0.6.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting PyYAML<6.1,>=3.10 (from awscli==1.29.21)\n",
      "  Obtaining dependency information for PyYAML<6.1,>=3.10 from https://files.pythonhosted.org/packages/29/61/bf33c6c85c55bc45a29eee3195848ff2d518d84735eb0e2d8cb42e0d285e/PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting colorama<0.4.5,>=0.2.5 (from awscli==1.29.21)\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting rsa<4.8,>=3.1.2 (from awscli==1.29.21)\n",
      "  Using cached rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.31.21)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore==1.31.21)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore==1.31.21)\n",
      "  Obtaining dependency information for urllib3<1.27,>=1.25.4 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore==1.31.21)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<4.8,>=3.1.2->awscli==1.29.21)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "Using cached s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
      "Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Installing collected packages: urllib3, six, PyYAML, pyasn1, jmespath, docutils, colorama, rsa, python-dateutil, botocore, s3transfer, boto3, awscli\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyasn1\n",
      "    Found existing installation: pyasn1 0.5.0\n",
      "    Uninstalling pyasn1-0.5.0:\n",
      "      Successfully uninstalled pyasn1-0.5.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: docutils\n",
      "    Found existing installation: docutils 0.16\n",
      "    Uninstalling docutils-0.16:\n",
      "      Successfully uninstalled docutils-0.16\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.4\n",
      "    Uninstalling colorama-0.4.4:\n",
      "      Successfully uninstalled colorama-0.4.4\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.7.2\n",
      "    Uninstalling rsa-4.7.2:\n",
      "      Successfully uninstalled rsa-4.7.2\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.21\n",
      "    Uninstalling botocore-1.31.21:\n",
      "      Successfully uninstalled botocore-1.31.21\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.6.2\n",
      "    Uninstalling s3transfer-0.6.2:\n",
      "      Successfully uninstalled s3transfer-0.6.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.28.21\n",
      "    Uninstalling boto3-1.28.21:\n",
      "      Successfully uninstalled boto3-1.28.21\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.29.21\n",
      "    Uninstalling awscli-1.29.21:\n",
      "      Successfully uninstalled awscli-1.29.21\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.20.5 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\n",
      "sparkmagic 0.20.5 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.0.3 which is incompatible.\n",
      "sphinx 7.0.1 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.1 awscli-1.29.21 boto3-1.28.21 botocore-1.31.21 colorama-0.4.4 docutils-0.16 jmespath-1.0.1 pyasn1-0.5.0 python-dateutil-2.8.2 rsa-4.7.2 s3transfer-0.6.2 six-1.16.0 urllib3-1.26.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opensearch-py==2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0) (1.26.16)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0) (2.31.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py==2.2.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.2.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py==2.2.0) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: requests-aws4auth in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.2.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-aws4auth) (2.31.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests-aws4auth) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->requests-aws4auth) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: opensearch-py in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (1.26.16)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2.31.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from opensearch-py) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.4.0->opensearch-py) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --no-build-isolation --force-reinstall \\\n",
    "    ./bedrock-python-sdk/awscli-*-py3-none-any.whl \\\n",
    "    ./bedrock-python-sdk/boto3-*-py3-none-any.whl \\\n",
    "   ./bedrock-python-sdk/botocore-*-py3-none-any.whl\n",
    "%pip install --quiet langchain==0.0.249 \"pypdf>=3.8,<4\"\n",
    "%pip install opensearch-py==2.2.0\n",
    "%pip install requests\n",
    "%pip install requests-aws4auth\n",
    "%pip install opensearch-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0390161-136d-4f24-9524-a6acc5c985cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "import logging, sys\n",
    "from typing import List\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.processing import ScriptProcessor, FrameworkProcessor\n",
    "from sagemaker.processing import ProcessingInput\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343808f",
   "metadata": {},
   "source": [
    "Change the parameters if you would like to scrape a different website for data, customize chunk size etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8576c1-56c5-44f6-b5f6-ff60e4faba67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:39:59,368,1200253552,MainProcess,INFO,aws_role=arn:aws:iam::506556589049:role/LLMHackathonAppIAMRole, aws_region=us-west-2, bucket=sagemaker-us-west-2-506556589049, source bucket=arxiv-docs-21082003-tmp\n"
     ]
    }
   ],
   "source": [
    "# global constants\n",
    "APP_NAME = \"llm-rag-hackathon\"\n",
    "DATA_DIR = \"data\"\n",
    "MAX_OS_DOCS_PER_PUT = 500\n",
    "IMAGE = \"load-data-opensearch-custom\"\n",
    "IMAGE_TAG = \"latest\"\n",
    "CHUNK_SIZE_FOR_DOC_SPLIT = 600\n",
    "CHUNK_OVERLAP_FOR_DOC_SPLIT = 20\n",
    "CREATE_OS_INDEX_HINT_FILE = \"_create_index_hint\"\n",
    "SOURCE_BUCKET = \"arxiv-docs-21082003-tmp\"\n",
    "opensearch_index=\"llm-rag-hackathon\"\n",
    "embeddings_model_endpoint_name='amazon.titan-embed-g1-text-02'\n",
    "llm_model_id=\"amazon.titan-tg1-xlarge\"\n",
    "BEDROCK_ENDPOINT_URL = \"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "#BEDROCK_REGION = \"us-west-2\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n",
    "logger.info(f\"aws_role={aws_role}, aws_region={aws_region}, bucket={bucket}, source bucket={SOURCE_BUCKET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19d81d",
   "metadata": {},
   "source": [
    "### Read parameters from Cloud Formation stack\n",
    "\n",
    "Some of the resources needed for this notebook such as the Embeddings LLM model endpoint, the Amazon OpenSearch cluster are created outside of this notebook, typically through a cloud formation template. We now read the outputs and parameters of the cloud formation stack created from that template to get the value of these parameters. \n",
    "\n",
    "The stack name here should match the stack name you used when creating the cloud formation stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2350d55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if used a different name while creating the cloud formation stack then change this to match the name you used\n",
    "CFN_STACK_NAME = \"llm-rag-hackathon\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eda3dc",
   "metadata": {},
   "source": [
    "**If you did not use a cloud formation template for creating these resources then set the names of these resources manually in the code below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4be2b244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:40:21,775,credentials,MainProcess,INFO,Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "stacks = boto3.client('cloudformation').list_stacks()\n",
    "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d4af3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:40:25,375,4289696515,MainProcess,INFO,cfn outputs={'LambdaIAMRole': 'arn:aws:iam::506556589049:role/llm-rag-hackathon-LambdaIAMRole-1J43Z0X2OS8S0', 'OpenSourceDomainArn': 'arn:aws:es:us-west-2:506556589049:domain/opensearchservi-uxkrnagpaaic', 'SageMakerIAMRole': 'arn:aws:iam::506556589049:role/LLMHackathonAppIAMRole', 'OpenSearchDomainEndpoint': 'search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', 'SageMakerNotebookURL': 'https://console.aws.amazon.com/sagemaker/home?region=us-west-2#/notebook-instances/openNotebook/aws-llm-rag-hackathon?view=classic', 'Region': 'us-west-2', 'OpenSearchDomainName': 'opensearchservi-uxkrnagpaaic', 'LLMAppAPIEndpoint': 'https://qxtw8nfy00.execute-api.us-west-2.amazonaws.com/prod/', 'OpenSearchSecret': 'arn:aws:secretsmanager:us-west-2:506556589049:secret:OpenSearchSecret-llm-rag-hackathon-fe5x3F'}\n",
      "params={'ContainerImageURI': '506556589049.dkr.ecr.us-west-2.amazonaws.com/lambda-rag-api:latest', 'BedrockEndPointUrl': 'https://prod.us-west-2.frontend.bedrock.aws.dev', 'SageMakerNotebookName': 'aws-llm-rag-hackathon', 'LambdaFunctionName': 'LLMRagapp', 'OpenSearchUsername': 'opensearchuser', 'APIGatewayName': 'LLMRagAPIGW', 'OpenSearchPassword': '****', 'AppName': 'llm-rag-hackathon', 'SageMakerIAMRole': 'LLMHackathonAppIAMRole', 'BedRockRegion': 'us-west-2', 'OpenSearchIndexName': 'llm-rag-hackathon', 'ApiStageName': 'prod', 'APIStage': '/prod'}\n"
     ]
    }
   ],
   "source": [
    "def get_cfn_outputs(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "def get_cfn_parameters(stackname: str) -> List:\n",
    "    cfn = boto3.client('cloudformation')\n",
    "    params = {}\n",
    "    for param in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Parameters']:\n",
    "        params[param['ParameterKey']] = param['ParameterValue']\n",
    "    return params\n",
    "\n",
    "if stack_found is True:\n",
    "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
    "    params = get_cfn_parameters(CFN_STACK_NAME)\n",
    "    logger.info(f\"cfn outputs={outputs}\\nparams={params}\")\n",
    "    opensearch_domain_endpoint = f\"https://{outputs['OpenSearchDomainEndpoint']}\"\n",
    "else:\n",
    "    logger.info(f\"cloud formation stack {CFN_STACK_NAME} not found, set parameters manually here\")\n",
    "    # REPLACE THE \"placeholder\" WITH ACTUAL VALUES IF YOU CREATED THESE RESOURCES WITHOUT USING A CLOUD FORMATION TEMPLATE\n",
    "    opensearch_domain_endpoint = \"placeholder\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cf7f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load data into `OpenSearch`\n",
    "\n",
    "We are now ready to create scripts which will read data from the local directory, use langchain to create embeddings and then upload the embeddings into OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76998f14-cbd2-4684-b2b8-27ea920e727f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘src’: File exists\n",
      "mkdir: cannot create directory ‘data’: File exists\n",
      "mkdir: cannot create directory ‘scripts’: File exists\n",
      "mkdir: cannot create directory ‘container’: File exists\n",
      "mkdir: cannot create directory ‘container/utils’: File exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create directories for storing scripts and Dockerfile\n",
    "\"\"\"\n",
    "!mkdir src\n",
    "!mkdir data\n",
    "!mkdir scripts\n",
    "!mkdir container\n",
    "!mkdir container/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d722d7-0386-42de-8ca0-213fd0ab55ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/_create_index_hint\n",
      "upload: data/_create_index_hint to s3://arxiv-docs-21082003-tmp/_create_index_hint\n"
     ]
    }
   ],
   "source": [
    "# create a dummy file called _create_index to provide a hint for opensearch index creation\n",
    "# this is needed for Sagemaker Processing Job when there are multiple instance nodes\n",
    "# all running the same code for data ingestion but only one node needs to create the index\n",
    "!touch $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE\n",
    "\n",
    "!ls $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE\n",
    "\n",
    "# upload this data to S3, to be used when we run the Sagemaker Processing Job\n",
    "#!aws s3 cp --recursive $DATA_DIR/ s3://$bucket/$app_name/$DOMAIN\n",
    "\n",
    "#TODO - Ensure the Sagemaker role has access to the custom bucket. - Update IAM role. LLMAppsBlogIAMRole\n",
    "!aws s3 cp $DATA_DIR/$CREATE_OS_INDEX_HINT_FILE s3://$SOURCE_BUCKET/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353ee897",
   "metadata": {},
   "source": [
    "### Script to load data into OpenSearch\n",
    "\n",
    "This script puts everything together, it divides the documents into chunks, then uses the langchain package to create embeddings and then ingests the data into OpenSearch using `OpenSearchVectorSearch`. \n",
    "\n",
    "To keep things simple the chunks size is set to a fixed length of 500 tokens, with an overlap of 30 tokens. The langchain `OpenSearchVectorSearch` provides a wrapper over the `opensearch-py` package. It uses the `/_bulk` API endpoint for ingesting multiple records in a single PUT request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f60f43-f9dd-43a5-9e79-104a6f8ee867",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/load_data_into_opensearch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/load_data_into_opensearch.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# this is needed because the credentials.py and sm_helper.py\n",
    "# are in /code directory of the custom container we are going \n",
    "# to create for Sagemaker Processing Job\n",
    "sys.path.insert(1, '/code')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from itertools import repeat\n",
    "from functools import partial\n",
    "import sagemaker, boto3, json\n",
    "from typing import List, Tuple\n",
    "from sagemaker.session import Session\n",
    "from opensearchpy.client import OpenSearch\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# global constants\n",
    "MAX_OS_DOCS_PER_PUT = 500\n",
    "TOTAL_INDEX_CREATION_WAIT_TIME = 60\n",
    "PER_ITER_SLEEP_TIME = 5\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s,%(module)s,%(processName)s,%(levelname)s,%(message)s', level=logging.INFO, stream=sys.stderr)\n",
    "\n",
    "\n",
    "def check_if_index_exists(opensearch_domain_endpoint, index_name, region, host, http_auth) -> OpenSearch:\n",
    "    \n",
    "    aos_client = OpenSearch(\n",
    "        hosts = [{'host': opensearch_domain_endpoint.replace(\"https://\", \"\"), 'port': 443}],\n",
    "        http_auth = http_auth,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    exists = aos_client.indices.exists(index_name)\n",
    "    logger.info(f\"index_name={index_name}, exists={exists}\")\n",
    "    return exists\n",
    "\n",
    "    \n",
    "def process_shard(shard, embeddings_model_endpoint_name, aws_region, os_index_name, os_domain_ep, os_http_auth) -> int: \n",
    "    logger.info(f'Starting process_shard of {len(shard)} chunks.')\n",
    "    st = time.time()\n",
    "    embeddings = bedrock_embeddings\n",
    "    \n",
    "    docsearch = OpenSearchVectorSearch(index_name=os_index_name,\n",
    "                                       embedding_function=embeddings,\n",
    "                                       opensearch_url=os_domain_ep,\n",
    "                                       http_auth=os_http_auth,\n",
    "                                       is_aoss=False,\n",
    "                                       use_ssl = True,\n",
    "                                       verify_certs = True,\n",
    "                                       timeout = 300,\n",
    "                                       connection_class = RequestsHttpConnection)    \n",
    "    docsearch.add_documents(documents=shard)\n",
    "    et = time.time() - st\n",
    "    logger.info(f'Shard completed in {et} seconds.')\n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--opensearch-cluster-domain\", type=str, default=None)\n",
    "    parser.add_argument(\"--opensearch-secretid\", type=str, default=None)\n",
    "    parser.add_argument(\"--opensearch-index-name\", type=str, default=None)\n",
    "    parser.add_argument(\"--aws-region\", type=str, default=\"us-west-2\")\n",
    "    parser.add_argument(\"--embeddings-model-endpoint-name\", type=str, default=None)\n",
    "    parser.add_argument(\"--chunk-size-for-doc-split\", type=int, default=500)\n",
    "    parser.add_argument(\"--chunk-overlap-for-doc-split\", type=int, default=30)\n",
    "    parser.add_argument(\"--input-data-dir\", type=str, default=\"/opt/ml/processing/input_data\")\n",
    "    parser.add_argument(\"--process-count\", type=int, default=1)\n",
    "    parser.add_argument(\"--create-index-hint-file\", type=str, default=\"_create_index_hint\")\n",
    "    parser.add_argument(\"--bedrock-endpoint-url\", type=str, default=None)\n",
    "    parser.add_argument(\"--llm-model-id\", type=str, default=None)\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    logger.info(\"Received arguments {}\".format(args))\n",
    "    # list all the files\n",
    "    files = glob.glob(os.path.join(args.input_data_dir, \"*.*\"))\n",
    "    logger.info(f\"there are {len(files)} files to process in the {args.input_data_dir} folder\")\n",
    "    \n",
    "    loader = PyPDFDirectoryLoader(path=args.input_data_dir, silent_errors=True)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"], #split chunks at (1) paragraph, (2) line, (3) sentence, or (4) word, in that order\n",
    "        chunk_size=args.chunk_size_for_doc_split,\n",
    "        chunk_overlap=args.chunk_overlap_for_doc_split,\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    # Stage one: read all the docs, split them into chunks. \n",
    "    st = time.time() \n",
    "    logger.info('Loading documents ...')\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "    # avg_char_count_pre = avg_doc_length(docs)\n",
    "    # avg_char_count_post = avg_doc_length(docs)\n",
    "    # print(f'Average length among {len(docs)} documents loaded is {avg_char_count_pre} characters.')\n",
    "    # print(f'After the split we have {len(docs)} documents more than the original {len(files)}.')\n",
    "    # print(f'Average length among {len(docs)} documents (after split) is {avg_char_count_post} characters.')\n",
    "    \n",
    "    # add a custom metadata field, such as timestamp\n",
    "    for doc in docs:\n",
    "        doc.metadata['timestamp'] = time.time()\n",
    "        doc.metadata['embeddings_model'] = args.embeddings_model_endpoint_name\n",
    "    chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
    "    et = time.time() - st\n",
    "    logger.info(f'Time taken: {et} seconds. {len(chunks)} chunks generated') \n",
    "    \n",
    "    \n",
    "    db_shards = (len(chunks) // MAX_OS_DOCS_PER_PUT) + 1\n",
    "    logger.info(f'Loading chunks into vector store ... using {db_shards} shards')\n",
    "    st = time.time()\n",
    "    shards = np.array_split(chunks, db_shards)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    \n",
    "    #OpenSearch Auth\n",
    "    service = 'es'\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, args.aws_region, service, session_token=credentials.token)\n",
    "    \n",
    "    #Bedrock embeddings\n",
    "    BEDROCK_ENDPOINT_URL = args.bedrock_endpoint_url\n",
    "    BEDROCK_REGION = args.aws_region\n",
    "  \n",
    "    boto3_bedrock = boto3.client(\n",
    "         service_name='bedrock',\n",
    "         region_name=BEDROCK_REGION,\n",
    "         endpoint_url=BEDROCK_ENDPOINT_URL\n",
    "    )\n",
    "    \n",
    "    model_kwargs =  { \n",
    "        \"maxTokenCount\": 8192, \n",
    "        \"stopSequences\": [], \n",
    "        \"temperature\": 0, \n",
    "        \"topP\": 1 \n",
    "    }\n",
    "\n",
    "\n",
    "    llm = Bedrock(model_id=args.llm_model_id, client=boto3_bedrock, model_kwargs=model_kwargs)\n",
    "    bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock,model_id=args.embeddings_model_endpoint_name)\n",
    "    \n",
    "    # first check if index exists, if it does then call the add_documents function\n",
    "    # otherwise call the from_documents function which would first create the index\n",
    "    # and then do a bulk add. Both add_documents and from_documents do a bulk add\n",
    "    # but it is important to call from_documents first so that the index is created\n",
    "    # correctly for K-NN\n",
    "    logger.info(f'Checking if index exists')\n",
    "    index_exists = check_if_index_exists(args.opensearch_cluster_domain,\n",
    "                                         args.opensearch_index_name,\n",
    "                                         args.aws_region,\n",
    "                                         args.opensearch_cluster_domain,\n",
    "                                         awsauth)\n",
    "    \n",
    "    embeddings =  bedrock_embeddings\n",
    "    \n",
    "    \n",
    "    if index_exists is False:\n",
    "        # create an index if the create index hint file exists\n",
    "        path = os.path.join(args.input_data_dir, args.create_index_hint_file)\n",
    "        if os.path.isfile(path) is True:\n",
    "            logger.info(f\"index {args.opensearch_index_name} does not exist but {path} file is present so will create index\")\n",
    "            \n",
    "            # by default langchain would create a k-NN index and the embeddings would be ingested as a k-NN vector type\n",
    "            docsearch = OpenSearchVectorSearch.from_documents(index_name=args.opensearch_index_name,\n",
    "                                                              documents=shards[0],\n",
    "                                                              embedding=embeddings,\n",
    "                                                              opensearch_url=args.opensearch_cluster_domain,\n",
    "                                                              use_ssl = True,\n",
    "                                                              verify_certs = True,  \n",
    "                                                              timeout = 300,\n",
    "                                                              connection_class = RequestsHttpConnection,\n",
    "                                                              http_auth=awsauth)\n",
    "            # we now need to start the loop below for the second shard\n",
    "            shard_start_index = 1  \n",
    "        else:\n",
    "            logger.info(f\"index {args.opensearch_index_name} does not exist and {path} file is not present, \"\n",
    "                        f\"will wait for some other node to create the index\")\n",
    "            shard_start_index = 0\n",
    "            # start a loop to wait for index creation by another node\n",
    "            time_slept = 0\n",
    "            while True:\n",
    "                logger.info(f\"index {args.opensearch_index_name} still does not exist, sleeping...\")\n",
    "                time.sleep(PER_ITER_SLEEP_TIME)\n",
    "                index_exists = check_if_index_exists(args.opensearch_cluster_domain,\n",
    "                                                     args.opensearch_index_name,\n",
    "                                                     args.aws_region,\n",
    "                                                     args.opensearch_cluster_domain,\n",
    "                                                     awsauth)\n",
    "                if index_exists is True:\n",
    "                    logger.info(f\"index {args.opensearch_index_name} now exists\")\n",
    "                    break\n",
    "                time_slept += PER_ITER_SLEEP_TIME\n",
    "                if time_slept >= TOTAL_INDEX_CREATION_WAIT_TIME:\n",
    "                    logger.error(f\"time_slept={time_slept} >= {TOTAL_INDEX_CREATION_WAIT_TIME}, not waiting anymore for index creation\")\n",
    "                    break\n",
    "                \n",
    "    else:\n",
    "        logger.info(f\"index={args.opensearch_index_name} does exist, going to call add_documents\")\n",
    "        shard_start_index = 0\n",
    "        \n",
    "    with mp.Pool(processes = args.process_count) as pool:\n",
    "        results = pool.map(partial(process_shard,\n",
    "                                   embeddings_model_endpoint_name=args.embeddings_model_endpoint_name,\n",
    "                                   aws_region=args.aws_region,\n",
    "                                   os_index_name=args.opensearch_index_name,\n",
    "                                   os_domain_ep=args.opensearch_cluster_domain,\n",
    "                                   os_http_auth=awsauth),\n",
    "                           shards[shard_start_index:])\n",
    "    \n",
    "    t2 = time.time()\n",
    "    logger.info(f'run time in seconds: {t2-t1:.2f}')\n",
    "    logger.info(\"all done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8d27e3-b4fd-4d72-bb28-4334998c063e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the data in a OpenSearch index ( Local Mode )\n",
    "import subprocess\n",
    "def run_cmd(cmd: str) -> None:\n",
    "    \"\"\"\n",
    "    Run a shell command. This function exists because often it is \n",
    "    cumbersome to run a shell command that takes parameters which \n",
    "    are Python variables.\n",
    "    \"\"\"\n",
    "    MAX_OUTPUT_LEN = 20000\n",
    "    logger.info(f\"run_cmd, going to run cmd=\\\"{cmd}\\\"\")\n",
    "\n",
    "    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    errcode = p.returncode\n",
    "\n",
    "    # result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    out = out.decode(\"utf-8\") \n",
    "    err = err.decode(\"utf-8\")\n",
    "    if len(out) > MAX_OUTPUT_LEN:\n",
    "        out = out[:-MAX_OUTPUT_LEN]\n",
    "    if len(err) > MAX_OUTPUT_LEN:\n",
    "        err = err[:-MAX_OUTPUT_LEN]\n",
    "    logger.info(f\"errcode={errcode}, out={out}, err={err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ddd2f7-5fc0-46fb-b2cf-c5b40374641b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:41:56,310,2701951082,MainProcess,INFO,run_cmd, going to run cmd=\"python container/load_data_into_opensearch.py --opensearch-cluster-domain https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com --opensearch-index-name llm-rag-hackathon --aws-region us-west-2 --embeddings-model-endpoint-name amazon.titan-embed-g1-text-02 --chunk-size-for-doc-split 500 --chunk-overlap-for-doc-split 50 --input-data-dir data --create-index-hint-file _create_index_hint --bedrock-endpoint-url https://prod.us-west-2.frontend.bedrock.aws.dev --llm-model-id amazon.titan-tg1-xlarge --process-count 2\n",
      "\"\n",
      "2023-09-08 18:43:24,127,2701951082,MainProcess,INFO,errcode=1, out=sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      ", err=2023-09-08 18:41:59,968,load_data_into_opensearch,MainProcess,INFO,Received arguments Namespace(opensearch_cluster_domain='https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', opensearch_secretid=None, opensearch_index_name='llm-rag-hackathon', aws_region='us-west-2', embeddings_model_endpoint_name='amazon.titan-embed-g1-text-02', chunk_size_for_doc_split=500, chunk_overlap_for_doc_split=50, input_data_dir='data', process_count=2, create_index_hint_file='_create_index_hint', bedrock_endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev', llm_model_id='amazon.titan-tg1-xlarge')\n",
      "2023-09-08 18:41:59,969,load_data_into_opensearch,MainProcess,INFO,there are 23 files to process in the data folder\n",
      "2023-09-08 18:41:59,969,load_data_into_opensearch,MainProcess,INFO,Loading documents ...\n",
      "2023-09-08 18:42:13,401,load_data_into_opensearch,MainProcess,INFO,Time taken: 13.431850671768188 seconds. 1644 chunks generated\n",
      "2023-09-08 18:42:13,401,load_data_into_opensearch,MainProcess,INFO,Loading chunks into vector store ... using 4 shards\n",
      "2023-09-08 18:42:13,432,credentials,MainProcess,INFO,Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "2023-09-08 18:42:13,464,credentials,MainProcess,INFO,Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "2023-09-08 18:42:13,502,load_data_into_opensearch,MainProcess,INFO,Checking if index exists\n",
      "2023-09-08 18:42:13,565,load_data_into_opensearch,MainProcess,INFO,index_name=llm-rag-hackathon, exists=False\n",
      "2023-09-08 18:42:13,565,load_data_into_opensearch,MainProcess,INFO,index llm-rag-hackathon does not exist but data/_create_index_hint file is present so will create index\n",
      "2023-09-08 18:42:48,630,base,MainProcess,WARNING,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:404 request:0.052s]\n",
      "2023-09-08 18:42:48,839,base,MainProcess,INFO,PUT https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.208s]\n",
      "2023-09-08 18:42:48,994,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.084s]\n",
      "2023-09-08 18:42:49,097,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.047s]\n",
      "2023-09-08 18:42:49,185,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.038s]\n",
      "2023-09-08 18:42:49,279,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.038s]\n",
      "2023-09-08 18:42:49,388,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.047s]\n",
      "2023-09-08 18:42:49,495,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.044s]\n",
      "2023-09-08 18:42:49,597,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.043s]\n",
      "2023-09-08 18:42:49,628,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.018s]\n",
      "2023-09-08 18:42:49,689,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.059s]\n",
      "2023-09-08 18:42:49,762,load_data_into_opensearch,ForkPoolWorker-2,INFO,Starting process_shard of 411 chunks.\n",
      "2023-09-08 18:42:49,762,load_data_into_opensearch,ForkPoolWorker-1,INFO,Starting process_shard of 411 chunks.\n",
      "2023-09-08 18:42:49,876,load_data_into_opensearch,ForkPoolWorker-2,INFO,Starting process_shard of 411 chunks.\n",
      "2023-09-08 18:43:22,152,base,ForkPoolWorker-1,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.035s]\n",
      "2023-09-08 18:43:22,210,base,ForkPoolWorker-2,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.034s]\n",
      "2023-09-08 18:43:22,288,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.053s]\n",
      "2023-09-08 18:43:22,335,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.054s]\n",
      "2023-09-08 18:43:22,382,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.047s]\n",
      "2023-09-08 18:43:22,456,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.050s]\n",
      "2023-09-08 18:43:22,484,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.046s]\n",
      "2023-09-08 18:43:22,594,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.058s]\n",
      "2023-09-08 18:43:22,598,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.064s]\n",
      "2023-09-08 18:43:22,691,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.049s]\n",
      "2023-09-08 18:43:22,704,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.050s]\n",
      "2023-09-08 18:43:22,776,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.041s]\n",
      "2023-09-08 18:43:22,798,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.040s]\n",
      "2023-09-08 18:43:22,864,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.044s]\n",
      "2023-09-08 18:43:22,898,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.049s]\n",
      "2023-09-08 18:43:22,900,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.027s]\n",
      "2023-09-08 18:43:23,018,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.048s]\n",
      "2023-09-08 18:43:23,061,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.033s]\n",
      "2023-09-08 18:43:23,066,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.166s]\n",
      "2023-09-08 18:43:23,075,load_data_into_opensearch,ForkPoolWorker-1,INFO,Shard completed in 33.31184363365173 seconds.\n",
      "2023-09-08 18:43:23,087,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.026s]\n",
      "2023-09-08 18:43:23,098,load_data_into_opensearch,ForkPoolWorker-2,INFO,Shard completed in 33.221762895584106 seconds.\n",
      "multiprocessing.pool.RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py\", line 444, in _error_catcher\n",
      "    yield\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py\", line 567, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py\", line 533, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/http/client.py\", line 482, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/http/client.py\", line 633, in _safe_read\n",
      "    raise IncompleteRead(data, amt-len(data))\n",
      "http.client.IncompleteRead: IncompleteRead(16193 bytes read, 1299 more expected)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/response.py\", line 99, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py\", line 566, in read\n",
      "    with self._error_catcher():\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/urllib3/response.py\", line 461, in _error_catcher\n",
      "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
      "urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(16193 bytes read, 1299 more expected)', IncompleteRead(16193 bytes read, 1299 more expected))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/embeddings/bedrock.py\", line 126, in _embedding_func\n",
      "    response_body = json.loads(response.get(\"body\").read())\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/response.py\", line 104, in read\n",
      "    raise ResponseStreamingError(error=e)\n",
      "botocore.exceptions.ResponseStreamingError: An error occurred while reading from response stream: ('Connection broken: IncompleteRead(16193 bytes read, 1299 more expected)', IncompleteRead(16193 bytes read, 1299 more expected))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/multiprocessing/pool.py\", line 48, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/home/ec2-user/SageMaker/llm-rag-hackathon/opensearch-data-ingestion/container/load_data_into_opensearch.py\", line 71, in process_shard\n",
      "    docsearch.add_documents(documents=shard)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/vectorstores/base.py\", line 104, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/vectorstores/opensearch_vector_search.py\", line 378, in add_texts\n",
      "    embeddings = self.embedding_function.embed_documents(list(texts))\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/embeddings/bedrock.py\", line 148, in embed_documents\n",
      "    response = self._embedding_func(text)\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/langchain/embeddings/bedrock.py\", line 129, in _embedding_func\n",
      "    raise ValueError(f\"Error raised by inference endpoint: {e}\")\n",
      "ValueError: Error raised by inference endpoint: An error occurred while reading from response stream: ('Connection broken: IncompleteRead(16193 bytes read, 1299 more expected)', IncompleteRead(16193 bytes read, 1299 more expected))\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/llm-rag-hackathon/opensearch-data-ingestion/container/load_data_into_opensearch.py\", line 220, in <module>\n",
      "    results = pool.map(partial(process_shard,\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/multiprocessing/pool.py\", line 367, in map\n",
      "    return self._map_async(func, iterable, mapstar, chunksize).get()\n",
      "  File \"/home/ec2-user/anaconda3/envs/python3/lib/python3.10/multiprocessing/pool.py\", line 774, in get\n",
      "    raise self._value\n",
      "ValueError: Error raised by inference endpoint: An error occurred while reading from response stream: ('Connection broken: IncompleteRead(16193 bytes read, 1299 more expected)', IncompleteRead(16193 bytes read, 1299 more expected))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd = f\"\"\"python container/load_data_into_opensearch.py --opensearch-cluster-domain {opensearch_domain_endpoint} \\\n",
    "--opensearch-index-name {opensearch_index} \\\n",
    "--aws-region {aws_region} \\\n",
    "--embeddings-model-endpoint-name {embeddings_model_endpoint_name} \\\n",
    "--chunk-size-for-doc-split 500 \\\n",
    "--chunk-overlap-for-doc-split 50 \\\n",
    "--input-data-dir {DATA_DIR} \\\n",
    "--create-index-hint-file {CREATE_OS_INDEX_HINT_FILE} \\\n",
    "--bedrock-endpoint-url {BEDROCK_ENDPOINT_URL} \\\n",
    "--llm-model-id {llm_model_id} \\\n",
    "--process-count 2\n",
    "\"\"\"\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdac398",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the data in a `OpenSearch` index via SageMaker Processing Job (Distributed mode)\n",
    "\n",
    "We now have a working script that is able to ingest data into an OpenSearch index. But for this to work for massive amounts of data we need to scale up the processing by running this code in a distributed fashion. We will do this using Sagemkaer Processing Job. This involves the following steps:\n",
    "\n",
    "1. Create a custom container in which we will install the `langchain` and `opensearch-py` packges and then upload this container image to Amazon Elastic Container Registry (ECR).\n",
    "2. Use the Sagemaker `ScriptProcessor` class to create a Sagemaker Processing job that will run on multiple nodes.\n",
    "    - The data files available in S3 are automatically distributed across in the Sagemaker Processing Job instances by setting `s3_data_distribution_type='ShardedByS3Key'` as part of the `ProcessingInput` provided to the processing job.\n",
    "    - Each node processes a subset of the files and this brings down the overall time required to ingest the data into Opensearch.\n",
    "    - Each node also uses Python `multiprocessing` to internally also parallelize the file processing. Thus, **there are two levels of parallelization happening, one at the cluster level where individual nodes are distributing the work (files) amongst themselves and another at the node level where the files in a node are also split between multiple processes running on the node**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcd8ee",
   "metadata": {},
   "source": [
    "### Create custom container\n",
    "\n",
    "We will now create a container locally and push the container image to ECR. **The container creation process takes about 1 minute**.\n",
    "\n",
    "1. The container include all the Python packages we need i.e. `langchain`, `opensearch-py`, `sagemaker` and `beautifulsoup4`.\n",
    "1. The container also includes the `credentials.py` script for retrieving credentials from Secrets Manager and `sm_helper.py` for helping to create SageMaker endpoint classes that langchain uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f719427-a448-43f1-90f3-4ad9259e7b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting container/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile container/Dockerfile\n",
    "\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "RUN apt-get -y update && apt-get install -y --no-install-recommends \\\n",
    "         curl \\\n",
    "         unzip \\\n",
    "         python3-pip \\\n",
    "         python3-setuptools \\\n",
    "         nginx \\\n",
    "         ca-certificates \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN ln -s /usr/bin/python3 /usr/bin/python\n",
    "RUN ln -s /usr/bin/pip3 /usr/bin/pip\n",
    "\n",
    "# pip leaves the install caches populated which uses a \n",
    "# significant amount of space. These optimizations save a fair \n",
    "# amount of space in the image, which reduces start up time.\n",
    "\n",
    "# download bedrock dependencies\n",
    "RUN curl https://d2eo22ngex1n9g.cloudfront.net/Documentation/SDK/bedrock-python-sdk.zip --output bedrock-python-sdk.zip && unzip bedrock-python-sdk.zip -d bedrock-python-sdk\n",
    "RUN pip --no-cache-dir install langchain==0.0.249 opensearch-py==2.2.0 sagemaker==2.148.0 beautifulsoup4==4.12.2 \"faiss-cpu>=1.7,<2\" \"pypdf>=3.8,<4\" requests requests-aws4auth\n",
    "RUN pip install --no-build-isolation --force-reinstall \\\n",
    "    ./bedrock-python-sdk/awscli-*-py3-none-any.whl \\\n",
    "    ./bedrock-python-sdk/boto3-*-py3-none-any.whl \\\n",
    "   ./bedrock-python-sdk/botocore-*-py3-none-any.whl\n",
    "\n",
    "ADD utils /code/utils\n",
    "COPY load_data_into_opensearch.py /code/\n",
    "RUN rm -rf bedrock-python-sdk\n",
    "\n",
    "\n",
    "# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n",
    "# output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n",
    "# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n",
    "# PATH so that the train and serve programs are found when the container is invoked.\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d18ac0-5deb-4fb6-9054-35816b1840b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/build_and_push.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/build_and_push.sh\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "# The argument to this script are the path to the Dockerfile, the image name and tag and the aws-region\n",
    "# in which the container is to be created. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "\n",
    "# override the built-in echo so that we can have a nice timestamped trace\n",
    "echo () {\n",
    "    builtin echo \"$(date +'[%m-%d %H:%M:%S]'):\" \"$@\"\n",
    "}\n",
    "\n",
    "if [ \"$#\" -eq 4 ]; then\n",
    "    dlc_account_id=$(aws sts get-caller-identity | jq .Account)\n",
    "    path_to_dockerfile=$1\n",
    "    image=$2\n",
    "    tag=$3\n",
    "    region=$4\n",
    "    \n",
    "else\n",
    "    echo \"missing mandatory command line arguments, see usage...\"\n",
    "    echo \"usage: $0 </path/to/Dockerfile> $1 <image-repo> $2 <image-tag> $3 <aws-region>\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Get the account number associated with the current IAM credentials\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    exit 255\n",
    "fi\n",
    "\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${image}:${tag}\"\n",
    "echo the full image name would be ${fullname}\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --region ${region} --repository-names \"${image}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]; then\n",
    "    echo \"creating ECR repository : ${fullname} \"\n",
    "    aws ecr create-repository --region ${region} --repository-name \"${image}\" > /dev/null\n",
    "else\n",
    "    echo \"${image} repo already exists in ECR\"\n",
    "fi\n",
    "\n",
    "# move to path of dockerfile\n",
    "cd ${path_to_dockerfile}\n",
    "\n",
    "# get credentials to login to ECR and, build and tag the image\n",
    "# note the use of DOCKER_BUILDKIT=1, this is needed for some mount instructions in the Dockerfile\n",
    "echo \"going to start a docker build, image=${image}, using Dockerfile=${path_to_dockerfile}\"\n",
    "aws ecr get-login-password --region ${region} \\\n",
    "| docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\n",
    "DOCKER_BUILDKIT=1 docker build . -t ${image}  --build-arg dlc_account_id=${dlc_account_id} --build-arg region=${region}\n",
    "docker tag ${image} ${fullname}\n",
    "echo ${image} created\n",
    "\n",
    "# push the image to ECR\n",
    "cmd=\"aws ecr get-login-password --region ${region} | docker login --username AWS --password-stdin ${account}.dkr.ecr.${region}.amazonaws.com\"\n",
    "echo going to run \\\"${cmd}\\\" to login to ECR\n",
    "${cmd}\n",
    "\n",
    "cmd=\"docker push ${fullname}\"\n",
    "echo going to run \\\"${cmd}\\\" to push image to ecr\n",
    "${cmd}\n",
    "if [ $? -eq 0 ]; then\n",
    "    echo \"Amazon ECR URI: ${fullname}\"\n",
    "else\n",
    "    echo \"Error: Image ${fullname} build and push failed\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"all done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687202b3-3b51-4beb-8c3e-0269e85ba7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:46:13,838,383160684,MainProcess,INFO,region=us-west-2, account_id=506556589049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09-08 18:46:15]: the full image name would be 506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom:latest\n",
      "[09-08 18:46:16]: load-data-opensearch-custom repo already exists in ECR\n",
      "[09-08 18:46:16]: going to start a docker build, image=load-data-opensearch-custom, using Dockerfile=/home/ec2-user/SageMaker/llm-rag-hackathon/opensearch-data-ingestion/container\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      " => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.70kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim-buster  0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.70kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim-buster  0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.70kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim-buster  0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.70kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.9-slim-buster  0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (15/15) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.0s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 1.70kB                                     0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.9-slim-buster  0.5s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.0s\n",
      "\u001b[0m\u001b[34m => => transferring context: 11.05kB                                       0.0s\n",
      "\u001b[0m\u001b[34m => [ 1/10] FROM docker.io/library/python:3.9-slim-buster@sha256:320a7a42  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 2/10] RUN apt-get -y update && apt-get install -y --no-insta  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 3/10] RUN ln -s /usr/bin/python3 /usr/bin/python              0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 4/10] RUN ln -s /usr/bin/pip3 /usr/bin/pip                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 5/10] RUN curl https://d2eo22ngex1n9g.cloudfront.net/Documen  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 6/10] RUN pip --no-cache-dir install langchain==0.0.249 open  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 7/10] RUN pip install --no-build-isolation --force-reinstall  0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 8/10] ADD utils /code/utils                                   0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [ 9/10] COPY load_data_into_opensearch.py /code/                0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [10/10] RUN rm -rf bedrock-python-sdk                           0.0s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.0s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.0s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:f058ba5e40406942944bfe64f0d69ca33afb250d94716  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/load-data-opensearch-custom             0.0s\n",
      "\u001b[0m\u001b[?25h[09-08 18:46:18]: load-data-opensearch-custom created\n",
      "[09-08 18:46:18]: going to run \"aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 506556589049.dkr.ecr.us-west-2.amazonaws.com\" to login to ECR\n",
      "\n",
      "Unknown options: |,docker,login,--username,AWS,--password-stdin,506556589049.dkr.ecr.us-west-2.amazonaws.com\n",
      "[09-08 18:46:18]: going to run \"docker push 506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom:latest\" to push image to ecr\n",
      "The push refers to repository [506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom]\n",
      "\n",
      "\u001b[1Ba6cca749: Preparing \n",
      "\u001b[1B81f0cde4: Preparing \n",
      "\u001b[1B48daf434: Preparing \n",
      "\u001b[1Babc8d197: Preparing \n",
      "\u001b[1B13e45d73: Preparing \n",
      "\u001b[1B7b983a30: Preparing \n",
      "\u001b[1Bb8ecc7ae: Preparing \n",
      "\u001b[1Bde2472cc: Preparing \n",
      "\u001b[1B1ee42e71: Preparing \n",
      "\u001b[1Ba27560c1: Preparing \n",
      "\u001b[1B037e08b3: Preparing \n",
      "\u001b[1Beede8d6e: Preparing \n",
      "\u001b[1B55769c5e: Preparing \n",
      "\u001b[1B8a51359d: Layer already exists \u001b[12A\u001b[2K\u001b[8A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:739f8f22aeae80721d20d415c7deaaf7831de5e84edcfa25f185fa1f12d785cf size: 3255\n",
      "[09-08 18:46:18]: Amazon ECR URI: 506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom:latest\n",
      "[09-08 18:46:18]: all done\n"
     ]
    }
   ],
   "source": [
    "# Run script to build docker custom containe image and push it to ECR \n",
    "# Set region and sagemaker URI variables \n",
    "session = boto3.session.Session()\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "logger.info(f\"region={aws_region}, account_id={account_id}\")\n",
    "!bash scripts/build_and_push.sh $(pwd)/container $IMAGE $IMAGE_TAG $aws_region"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad4b50",
   "metadata": {},
   "source": [
    "### Create and run the Sagemaker Processing Job\n",
    "\n",
    "Now we will run the Sagemaker Processing Job to ingest the data into OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f45356-13fe-49f9-b2a0-09ac9dd54b62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:46:29,634,1792381935,MainProcess,INFO,base_job_name=llm-rag-hackathon-job, tags=[{'Key': 'data', 'Value': 'embeddings-for-llm-apps'}], image_uri=506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom:latest, instance_type=ml.m5.xlarge, instance_count=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:46:29,745,1792381935,MainProcess,INFO,creating an opensearch index with name=llm-rag-hackathon\n",
      "2023-09-08 18:46:30,127,session,MainProcess,INFO,Creating processing-job with name llm-rag-hackathon-job-2023-09-08-18-46-29-746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[32m2023-09-08 18:50:40,124,load_data_into_opensearch,MainProcess,INFO,Received arguments Namespace(opensearch_cluster_domain='https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', opensearch_secretid=None, opensearch_index_name='llm-rag-hackathon', aws_region='us-west-2', embeddings_model_endpoint_name='amazon.titan-embed-g1-text-02', chunk_size_for_doc_split=600, chunk_overlap_for_doc_split=20, input_data_dir='/opt/ml/processing/input_data', process_count=2, create_index_hint_file='_create_index_hint', bedrock_endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev', llm_model_id='amazon.titan-tg1-xlarge')\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:40,124,load_data_into_opensearch,MainProcess,INFO,there are 8 files to process in the /opt/ml/processing/input_data folder\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:40,124,load_data_into_opensearch,MainProcess,INFO,Loading documents ...\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:40,169,load_data_into_opensearch,MainProcess,INFO,Received arguments Namespace(opensearch_cluster_domain='https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', opensearch_secretid=None, opensearch_index_name='llm-rag-hackathon', aws_region='us-west-2', embeddings_model_endpoint_name='amazon.titan-embed-g1-text-02', chunk_size_for_doc_split=600, chunk_overlap_for_doc_split=20, input_data_dir='/opt/ml/processing/input_data', process_count=2, create_index_hint_file='_create_index_hint', bedrock_endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev', llm_model_id='amazon.titan-tg1-xlarge')\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:40,169,load_data_into_opensearch,MainProcess,INFO,there are 8 files to process in the /opt/ml/processing/input_data folder\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:40,169,load_data_into_opensearch,MainProcess,INFO,Loading documents ...\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:40,205,load_data_into_opensearch,MainProcess,INFO,Received arguments Namespace(opensearch_cluster_domain='https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', opensearch_secretid=None, opensearch_index_name='llm-rag-hackathon', aws_region='us-west-2', embeddings_model_endpoint_name='amazon.titan-embed-g1-text-02', chunk_size_for_doc_split=600, chunk_overlap_for_doc_split=20, input_data_dir='/opt/ml/processing/input_data', process_count=2, create_index_hint_file='_create_index_hint', bedrock_endpoint_url='https://prod.us-west-2.frontend.bedrock.aws.dev', llm_model_id='amazon.titan-tg1-xlarge')\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:40,206,load_data_into_opensearch,MainProcess,INFO,there are 7 files to process in the /opt/ml/processing/input_data folder\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:40,206,load_data_into_opensearch,MainProcess,INFO,Loading documents ...\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,165,load_data_into_opensearch,MainProcess,INFO,Time taken: 3.9954028129577637 seconds. 360 chunks generated\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,165,load_data_into_opensearch,MainProcess,INFO,Loading chunks into vector store ... using 1 shards\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,232,load_data_into_opensearch,MainProcess,INFO,Checking if index exists\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,264,base,MainProcess,INFO,HEAD https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.031s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,265,load_data_into_opensearch,MainProcess,INFO,index_name=llm-rag-hackathon, exists=True\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,265,load_data_into_opensearch,MainProcess,INFO,index=llm-rag-hackathon does exist, going to call add_documents\u001b[0m\n",
      "\u001b[35m2023-09-08 18:50:44,303,load_data_into_opensearch,ForkPoolWorker-1,INFO,Starting process_shard of 360 chunks.\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,704,load_data_into_opensearch,MainProcess,INFO,Time taken: 3.4981632232666016 seconds. 315 chunks generated\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,704,load_data_into_opensearch,MainProcess,INFO,Loading chunks into vector store ... using 1 shards\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,772,load_data_into_opensearch,MainProcess,INFO,Checking if index exists\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,829,base,MainProcess,INFO,HEAD https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.056s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,829,load_data_into_opensearch,MainProcess,INFO,index_name=llm-rag-hackathon, exists=True\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,829,load_data_into_opensearch,MainProcess,INFO,index=llm-rag-hackathon does exist, going to call add_documents\u001b[0m\n",
      "\u001b[34m2023-09-08 18:50:43,872,load_data_into_opensearch,ForkPoolWorker-1,INFO,Starting process_shard of 315 chunks.\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,389,load_data_into_opensearch,MainProcess,INFO,Time taken: 7.2644641399383545 seconds. 713 chunks generated\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,389,load_data_into_opensearch,MainProcess,INFO,Loading chunks into vector store ... using 2 shards\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,455,load_data_into_opensearch,MainProcess,INFO,Checking if index exists\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,487,base,MainProcess,INFO,HEAD https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.031s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,487,load_data_into_opensearch,MainProcess,INFO,index_name=llm-rag-hackathon, exists=True\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,487,load_data_into_opensearch,MainProcess,INFO,index=llm-rag-hackathon does exist, going to call add_documents\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,525,load_data_into_opensearch,ForkPoolWorker-1,INFO,Starting process_shard of 357 chunks.\u001b[0m\n",
      "\u001b[32m2023-09-08 18:50:47,528,load_data_into_opensearch,ForkPoolWorker-2,INFO,Starting process_shard of 356 chunks.\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:08,937,base,ForkPoolWorker-1,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.022s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,045,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.046s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,152,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.046s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,252,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.041s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,351,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.040s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,464,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.051s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,524,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.030s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,574,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.049s]\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,579,load_data_into_opensearch,ForkPoolWorker-1,INFO,Shard completed in 25.706982135772705 seconds.\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,586,load_data_into_opensearch,MainProcess,INFO,run time in seconds: 25.88\u001b[0m\n",
      "\u001b[34m2023-09-08 18:51:09,586,load_data_into_opensearch,MainProcess,INFO,all done\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:11,948,base,ForkPoolWorker-1,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.020s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,070,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.058s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,175,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.045s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,276,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.043s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,384,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.048s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,486,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.043s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,596,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.049s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,638,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.024s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,694,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.056s]\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,701,load_data_into_opensearch,ForkPoolWorker-1,INFO,Shard completed in 28.39700484275818 seconds.\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,706,load_data_into_opensearch,MainProcess,INFO,run time in seconds: 28.54\u001b[0m\n",
      "\u001b[35m2023-09-08 18:51:12,707,load_data_into_opensearch,MainProcess,INFO,all done\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:14,610,base,ForkPoolWorker-1,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.021s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:14,735,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.060s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:14,841,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.044s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:14,940,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.040s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,051,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.055s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,118,base,ForkPoolWorker-2,INFO,GET https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon [status:200 request:0.024s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,156,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.042s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,225,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.043s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,257,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.040s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,293,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.022s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,336,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.051s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,367,base,ForkPoolWorker-1,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.073s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,373,load_data_into_opensearch,ForkPoolWorker-1,INFO,Shard completed in 27.84780764579773 seconds.\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,434,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.040s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,537,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.045s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,633,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.038s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,738,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.046s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,771,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/_bulk [status:200 request:0.020s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,813,base,ForkPoolWorker-2,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_refresh [status:200 request:0.042s]\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,820,load_data_into_opensearch,ForkPoolWorker-2,INFO,Shard completed in 28.291717290878296 seconds.\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,826,load_data_into_opensearch,MainProcess,INFO,run time in seconds: 28.44\u001b[0m\n",
      "\u001b[32m2023-09-08 18:51:15,827,load_data_into_opensearch,MainProcess,INFO,all done\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 18:52:06,438,1792381935,MainProcess,INFO,processing job completed, total time taken=336.6925675868988s\n",
      "2023-09-08 18:52:06,472,1792381935,MainProcess,INFO,{'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://arxiv-docs-21082003-tmp', 'LocalPath': '/opt/ml/processing/input_data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-506556589049/llm-rag-hackathon-job-2023-09-08-18-46-29-746/input/code/load_data_into_opensearch.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingJobName': 'llm-rag-hackathon-job-2023-09-08-18-46-29-746', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 3, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '506556589049.dkr.ecr.us-west-2.amazonaws.com/load-data-opensearch-custom:latest', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/load_data_into_opensearch.py'], 'ContainerArguments': ['--opensearch-cluster-domain', 'https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com', '--opensearch-index-name', 'llm-rag-hackathon', '--aws-region', 'us-west-2', '--embeddings-model-endpoint-name', 'amazon.titan-embed-g1-text-02', '--chunk-size-for-doc-split', '600', '--chunk-overlap-for-doc-split', '20', '--input-data-dir', '/opt/ml/processing/input_data', '--create-index-hint-file', '_create_index_hint', '--bedrock-endpoint-url', 'https://prod.us-west-2.frontend.bedrock.aws.dev', '--llm-model-id', 'amazon.titan-tg1-xlarge', '--process-count', '2']}, 'RoleArn': 'arn:aws:iam::506556589049:role/LLMHackathonAppIAMRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:506556589049:processing-job/llm-rag-hackathon-job-2023-09-08-18-46-29-746', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2023, 9, 8, 18, 51, 19, 154000, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2023, 9, 8, 18, 50, 30, 54000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2023, 9, 8, 18, 51, 19, 558000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2023, 9, 8, 18, 46, 30, 236000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '7b687c83-233a-45f4-ba67-ac8ad360f581', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '7b687c83-233a-45f4-ba67-ac8ad360f581', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2027', 'date': 'Fri, 08 Sep 2023 18:52:05 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# setup the parameters for the job\n",
    "base_job_name = f\"{APP_NAME}-job\"\n",
    "tags = [{\"Key\": \"data\", \"Value\": \"embeddings-for-llm-apps\"}]\n",
    "\n",
    "# use the custom container we just created\n",
    "image_uri = f\"{account_id}.dkr.ecr.{aws_region}.amazonaws.com/{IMAGE}:{IMAGE_TAG}\"\n",
    "\n",
    "# instance type and count determined via trial and error: how much overall processing time\n",
    "# and what compute cost works best for your use-case\n",
    "instance_type = \"ml.m5.xlarge\"\n",
    "instance_count = 3\n",
    "logger.info(f\"base_job_name={base_job_name}, tags={tags}, image_uri={image_uri}, instance_type={instance_type}, instance_count={instance_count}\")\n",
    "\n",
    "# setup the ScriptProcessor with the above parameters\n",
    "processor = ScriptProcessor(base_job_name=base_job_name,\n",
    "                            image_uri=image_uri,\n",
    "                            role=aws_role,\n",
    "                            instance_type=instance_type,\n",
    "                            instance_count=instance_count,\n",
    "                            command=[\"python3\"],\n",
    "                            tags=tags)\n",
    "\n",
    "# setup input from S3, note the ShardedByS3Key, this ensures that \n",
    "# each instance gets a random and equal subset of the files in S3.\n",
    "inputs = [ProcessingInput(source=f\"s3://{SOURCE_BUCKET}\",\n",
    "                          destination='/opt/ml/processing/input_data',\n",
    "                          s3_data_distribution_type='ShardedByS3Key',\n",
    "                          s3_data_type='S3Prefix')]\n",
    "\n",
    "\n",
    "logger.info(f\"creating an opensearch index with name={opensearch_index}\")\n",
    "# ready to run the processing job\n",
    "st = time.time()\n",
    "processor.run(code=\"container/load_data_into_opensearch.py\",\n",
    "              inputs=inputs,\n",
    "              outputs=[],\n",
    "              arguments=[\"--opensearch-cluster-domain\", opensearch_domain_endpoint,\n",
    "                         \"--opensearch-index-name\", opensearch_index,\n",
    "                         \"--aws-region\", aws_region,\n",
    "                         \"--embeddings-model-endpoint-name\", embeddings_model_endpoint_name,\n",
    "                         \"--chunk-size-for-doc-split\", str(CHUNK_SIZE_FOR_DOC_SPLIT),\n",
    "                         \"--chunk-overlap-for-doc-split\", str(CHUNK_OVERLAP_FOR_DOC_SPLIT),\n",
    "                         \"--input-data-dir\", \"/opt/ml/processing/input_data\",\n",
    "                         \"--create-index-hint-file\", CREATE_OS_INDEX_HINT_FILE,\n",
    "                         \"--bedrock-endpoint-url\", BEDROCK_ENDPOINT_URL,\n",
    "                         \"--llm-model-id\", llm_model_id,\n",
    "                         \"--process-count\", \"2\"])\n",
    "time_taken = time.time() - st\n",
    "logger.info(f\"processing job completed, total time taken={time_taken}s\")\n",
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "logger.info(preprocessing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626157dd",
   "metadata": {},
   "source": [
    "## Step 3: Do a similarity search for for user input to documents (embeddings) in OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab2bc60-921e-4bb1-b7e7-db4c9b4165fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-08 19:01:51,158,credentials,MainProcess,INFO,Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "2023-09-08 19:01:51,401,base,MainProcess,INFO,POST https://search-opensearchservi-uxkrnagpaaic-5kqti6uvcu4b6npaxzqbyo7oom.us-west-2.es.amazonaws.com:443/llm-rag-hackathon/_search [status:200 request:0.051s]\n",
      "2023-09-08 19:01:51,404,3401234624,MainProcess,INFO,----------\n",
      "2023-09-08 19:01:51,405,3401234624,MainProcess,INFO,content=\"Acknowledgements\n",
      "WewouldliketospeciallythankWillSutherlandforthecollaborationrep ortedinLLS\n",
      "(someoftheresults ofwhich arereproduced here). Wealso wishto thankPeter Coles,\n",
      "Ed Copeland, Rod Davies, Richard Frewin, Josh Frieman, Martin Hend ry, Rocky\n",
      "42\",\n",
      "metadata=\"{'source': '/opt/ml/processing/input_data/astro-ph_9209003v1.The_Spectral_Index_in_the_CDM_Cosmogony.pdf', 'page': 42, 'timestamp': 1694199047.3668852, 'embeddings_model': 'amazon.titan-embed-g1-text-02'}\"\n",
      "2023-09-08 19:01:51,405,3401234624,MainProcess,INFO,----------\n",
      "2023-09-08 19:01:51,406,3401234624,MainProcess,INFO,content=\"model, thelowest allowed nhasthismaximumbias. Hencethemostoptimisticpower-\n",
      "law model, which may just ﬁt the clustering data, appears to be a nat ural inﬂation\n",
      "style model with values of n∼0.70 andb∼1.6. In addition to an improvement on\n",
      "clustering issues, such a model also seems to deal adequately with t he formation of\n",
      "structure (contrary to our more pessimistic assessment in LLS).\n",
      "A preference has been expressed for models with nin the range 0 .8 to 0.9 (see\n",
      "egSalopek 1992; Davis et al1992; Lucchin, Matarrese & Mollerach 1992), because\",\n",
      "metadata=\"{'source': '/opt/ml/processing/input_data/astro-ph_9209003v1.The_Spectral_Index_in_the_CDM_Cosmogony.pdf', 'page': 42, 'timestamp': 1694199047.3668852, 'embeddings_model': 'amazon.titan-embed-g1-text-02'}\"\n",
      "2023-09-08 19:01:51,407,3401234624,MainProcess,INFO,----------\n",
      "2023-09-08 19:01:51,407,3401234624,MainProcess,INFO,content=\"power-lawmassfunctionrequiresanunusually highlow-mas scut-oﬀ. Amoreprobable\n",
      "solution invokes the multi-mass models or perhaps a more com plex form for the mass\n",
      "function.\n",
      "ACKNOWLEDGEMENTS\n",
      "P.F. would like to knowledge the Natural Sciences and Engine ering Research Council\n",
      "(NSERC) for a post-graduate fellowship and operating grant support to D.L.W. This work\",\n",
      "metadata=\"{'source': '/opt/ml/processing/input_data/astro-ph_9206004v1.Dynamics_of_the_Intermediate_Age_Elliptical_LMC_Cluster_NGC_1978.pdf', 'page': 20, 'timestamp': 1694199043.6944487, 'embeddings_model': 'amazon.titan-embed-g1-text-02'}\"\n"
     ]
    }
   ],
   "source": [
    "# Do a similarity search for user input to documents ( embeddings) in OpenSearch\n",
    "import boto3\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "\n",
    "#OpenSearch Auth\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, aws_region, service, session_token=credentials.token)\n",
    "\n",
    "#Bedrock embeddings\n",
    "BEDROCK_ENDPOINT_URL = \"https://prod.us-west-2.frontend.bedrock.aws.dev\"\n",
    "BEDROCK_REGION = \"us-west-2\"\n",
    "\n",
    "\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    endpoint_url=BEDROCK_ENDPOINT_URL,\n",
    "    region_name=BEDROCK_REGION,\n",
    ")\n",
    "\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock,model_id=embeddings_model_endpoint_name)\n",
    "\n",
    "docsearch = OpenSearchVectorSearch(index_name=opensearch_index,\n",
    "                                   embedding_function=bedrock_embeddings,\n",
    "                                   opensearch_url=opensearch_domain_endpoint,\n",
    "                                   http_auth=awsauth,\n",
    "                                   is_aoss=False,\n",
    "                                   use_ssl = True,\n",
    "                                   verify_certs = True,\n",
    "                                   connection_class = RequestsHttpConnection) \n",
    "\n",
    "q = \"knowledge from language models\"\n",
    "#docs = docsearch.similarity_search(q, k=3, search_type=\"script_scoring\", space_type=\"cosinesimil\")\n",
    "docs = docsearch.similarity_search(q, k=3, search_type=\"approximate_search\", space_type=\"cosinesimil\")\n",
    "for doc in docs:\n",
    "    logger.info(\"----------\")\n",
    "    logger.info(f\"content=\\\"{doc.page_content}\\\",\\nmetadata=\\\"{doc.metadata}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797ddcc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "To avoid incurring future charges, delete the resources. You can do this by deleting the CloudFormation template used to create the IAM role and SageMaker notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb61b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "In this notebook we were able to see how to use the BEdrock Titam model to generate embeddings and then ingest those embeddings into OpenSearch and finally do a similarity search for user input to the documents (embeddings) stored in OpenSearch. We used langchain as an abstraction layer to talk to both the SageMaker Endpoint as well as OpenSearch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
